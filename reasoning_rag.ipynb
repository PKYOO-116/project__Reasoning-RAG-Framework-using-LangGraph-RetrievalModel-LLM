{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec535d77",
   "metadata": {},
   "source": [
    "Activate venv and install packages\n",
    "1. !source reasongraph/bin/activate </br>\n",
    "2. !pip install \"langchain>=0.3\" \"langgraph>=0.2\" qdrant-client sentence-transformers torch pydantic python-dotenv\n",
    "3. !pip install pymupdf tiktoken # Count number of tokens to check model compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b2e484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 21\n",
      "Context Length: 42,248\n",
      "Number of tokens: 8,686\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import tiktoken\n",
    "\n",
    "pdf_path = \"docs/Profile.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "text = \"\"\n",
    "for page in doc:\n",
    "    text += page.get_text()\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "num_tokens = len(enc.encode(text))\n",
    "\n",
    "print(f\"Number of pages: {len(doc)}\")\n",
    "print(f\"Context Length: {len(text):,}\")\n",
    "print(f\"Number of tokens: {num_tokens:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6510ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/GitHub/Private/_PKYOO-116/project__Reasoning-RAG-Framework-using-LangGraph/reasongraph/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "import fitz, torch, uuid, os, sys\n",
    "from pathlib import Path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ec64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RAG State\n",
    "class RAGState(BaseModel):\n",
    "    docs: List[str] = Field(default_factory=list)\n",
    "    chunks: List[str] = Field(default_factory=list)\n",
    "    query: Optional[str] = None\n",
    "    results: List[str] = Field(default_factory=list)\n",
    "    answer: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40a899e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text file and chunking\n",
    "def load_and_chunk(state: RAGState) -> RAGState:\n",
    "    folder = \"docs\"\n",
    "    texts = []\n",
    "\n",
    "    for file in Path(folder).rglob(\"*.pdf\"):\n",
    "        try:\n",
    "            with fitz.open(file) as pdf:\n",
    "                pdf_text = []\n",
    "                for page in pdf:\n",
    "                    pdf_text.append(page.get_text(\"text\"))\n",
    "                text = \"\\n\".join(pdf_text)\n",
    "                texts.append(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load PDF files: {file.name} ({e})\")\n",
    "\n",
    "    if not texts:\n",
    "        print('PDf files are unavailable from \"docs\" directory.')\n",
    "        return state\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "    chunks = splitter.split_text(\"\\n\".join(texts))\n",
    "\n",
    "    print(f\"Loaded {len(texts)} PDFs and divided into {len(chunks)} chunks\")\n",
    "    state.docs = texts\n",
    "    state.chunks = chunks\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ddcf178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU to run if possible\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3b8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Embedding models and load models to memory\n",
    "QWEN_MODEL = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "BGE_MODEL = \"BAAI/bge-m3\"\n",
    "qwen = SentenceTransformer(QWEN_MODEL, device=DEVICE)\n",
    "bge  = SentenceTransformer(BGE_MODEL,  device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4260ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vector dimension of each model (for qdrant collection)\n",
    "QWEN_DIM = qwen.get_sentence_embedding_dimension()\n",
    "BGE_DIM  = bge.get_sentence_embedding_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02bd476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant server connection configuration (localhost:6333)\n",
    "QDRANT_HOST = os.getenv(\"QDRANT_HOST\", \"localhost\")  # use default if no environment variables\n",
    "QDRANT_PORT = int(os.getenv(\"QDRANT_PORT\", \"6333\"))\n",
    "COLLECTION  = \"pkyoo_personal_docs_dualvec\"                # vector collection name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a66d1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant client reset and connection test\n",
    "try:\n",
    "    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT, timeout=60)\n",
    "    _ = client.get_collections()  # get collection lists to check connection\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect Qdrant. Check Qdrant Docker is running.\")\n",
    "    print(\"e.g., docker run -p 6333:6333 -v $(pwd)/qdrant_storage:/qdrant/storage qdrant/qdrant\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31de547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing = [c.name for c in client.get_collections().collections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "394de68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹCollection exists: pkyoo_personal_docs_dualvec\n"
     ]
    }
   ],
   "source": [
    "# Check collection existence and create collections if not\n",
    "if COLLECTION not in existing:\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION,\n",
    "        vectors_config={\n",
    "            \"qwen\": qmodels.VectorParams(size=QWEN_DIM, distance=qmodels.Distance.COSINE),\n",
    "            \"bge\":  qmodels.VectorParams(size=BGE_DIM,  distance=qmodels.Distance.COSINE)\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created collection: {COLLECTION}\")\n",
    "else:\n",
    "    print(f\"ℹCollection exists: {COLLECTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7791a21",
   "metadata": {},
   "source": [
    "Check http://localhost:6333/dashboard for local qdrant collections dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d7ced80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_store(state: RAGState, batch_size=128) -> RAGState:\n",
    "    # Check if chunks exist. if not, stop the process\n",
    "    if not state.chunks:\n",
    "        print(\"No chunks found. Run load_and_chunk() first.\")\n",
    "        return state\n",
    "\n",
    "    # Step 1: Encode(Embedding) document chunks using Qwen3-Embedding-0.6B\n",
    "    # Model converts each text chunk into a high-dimensional vector\n",
    "    print(\"Encoding with Qwen3-Embedding-0.6B ...\")\n",
    "    qwen_vecs = qwen.encode(\n",
    "        state.chunks,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        # Normalization helps cosine similarity work properly.\n",
    "        # NOT Standardization because it is not about statistics.\n",
    "        # Standardization would change the direction of vectors in space.\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    # Step 2: Encode the same chunks using bge-m3 model\n",
    "    print(\"Encoding with bge-m3 ...\")\n",
    "    bge_vecs = bge.encode(\n",
    "        state.chunks,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    # Step 3: Prepare data points for insertion into Qdrant\n",
    "    # Each point contains a unique ID (UUID) for both Qwen and BGE embedding vectors as a dictionary\n",
    "    # the original text payload and its chunk index\n",
    "    points = []\n",
    "    for i, (qv, bv) in enumerate(zip(qwen_vecs, bge_vecs)):\n",
    "        payload = {\n",
    "            \"text\": state.chunks[i],   # Original chunk text\n",
    "            \"chunk_index\": i           # Index of the chunk for traceability\n",
    "        }\n",
    "        points.append(qmodels.PointStruct(\n",
    "            id=str(uuid.uuid4()),      # Unique ID for each vector point\n",
    "            vector={\n",
    "                \"qwen\": qv.tolist(),   # Vector from Qwen3 model\n",
    "                \"bge\": bv.tolist()     # Vector from bge-m3 model\n",
    "            },\n",
    "            payload=payload\n",
    "        ))\n",
    "\n",
    "    # Step 4: Upload (upsert) all encoded points into the Qdrant collection\n",
    "    # 'upsert' means \"insert if new, update if already exists\"\n",
    "    # 'wait=True' ensures the operation completes before continuing\n",
    "    client.upsert(collection_name=COLLECTION, points=points, wait=True)\n",
    "\n",
    "    # Step 5: Log how many vectors were inserted\n",
    "    print(f\"Upserted {len(points)} vectors to '{COLLECTION}'\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_qdrant(state: RAGState, top_k: int = 5) -> RAGState:\n",
    "    # Retrieves top-k most relevant document chunks from Qdrant\n",
    "    # based on the query embedding generated by bge-m3 (RTEB model).\n",
    "\n",
    "    if not state.query:\n",
    "        print(\"No query provided in state.query\")\n",
    "        return state\n",
    "\n",
    "    print(\"Generating query embedding using bge-m3\")\n",
    "\n",
    "    # Step 1: Encode the query text using the retrieval embedding model (bge-m3)\n",
    "    query_vec = bge.encode(\n",
    "        [state.query],\n",
    "        normalize_embeddings=True  # cosine similarity requires normalized vectors\n",
    "    )[0]\n",
    "\n",
    "    # Step 2: Search in Qdrant using the 'bge' vector field\n",
    "    # The 'vector' argument must match the name used during embedding\n",
    "    print(f\"Searching Qdrant collection '{COLLECTION}' ...\")\n",
    "\n",
    "    search_result = client.search(\n",
    "        collection_name=COLLECTION,\n",
    "        query_vector=(\"bge\", query_vec),\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    # Step 3: Extract the retrieved texts (payloads)\n",
    "    results = [hit.payload[\"text\"] for hit in search_result]\n",
    "\n",
    "    # Step 4: Store retrieved chunks in RAG state\n",
    "    state.results = results\n",
    "\n",
    "    print(f\"Retrieved {len(results)} relevant chunks from Qdrant.\")\n",
    "    return state\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasongraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
