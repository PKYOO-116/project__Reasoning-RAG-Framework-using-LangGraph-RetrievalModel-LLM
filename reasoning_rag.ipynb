{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec535d77",
   "metadata": {},
   "source": [
    "Activate venv and install packages\n",
    "1. !source reasongraph/bin/activate </br>\n",
    "2. !pip install \"langchain>=0.3\" \"langgraph>=0.2\" qdrant-client sentence-transformers torch pydantic python-dotenv\n",
    "3. !pip install pymupdf tiktoken # Count number of tokens to check model compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6510ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/GitHub/Private/_PKYOO-116/project__Reasoning-RAG-Framework-using-LangGraph/reasongraph/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz, hashlib, torch, uuid, os, sys\n",
    "from langgraph.graph import StateGraph, END\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Dict, List, Optional\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ec64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGState(BaseModel):\n",
    "    docs: List[str] = Field(default_factory=list)\n",
    "    chunks: List[Dict[str, Any]] = Field(default_factory=list) # Includes metadata\n",
    "    query: Optional[str] = None\n",
    "    results: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "    answer: Optional[str] = None\n",
    "    retry_count: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a899e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    TOKEN_SPLIT_AVAILABLE = True\n",
    "except Exception:\n",
    "    TOKEN_SPLIT_AVAILABLE = False\n",
    "\n",
    "def _hash_text(t: str) -> str:\n",
    "    \"\"\"페이지 중복 제거용 해시 함수\"\"\"\n",
    "    return hashlib.sha256(t.strip().encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def load_and_chunk(state: RAGState, folder: str = \"docs\",\n",
    "                   chunk_size_tokens: int = 350,\n",
    "                   chunk_overlap_tokens: int = 50) -> RAGState:\n",
    "    texts, chunks, seen_hashes = [], [], set()\n",
    "\n",
    "    for file in Path(folder).rglob(\"*.pdf\"):\n",
    "        try:\n",
    "            with fitz.open(file) as pdf:\n",
    "                for i, page in enumerate(pdf, start=1):\n",
    "                    text = page.get_text(\"text\")\n",
    "                    if not text or not text.strip():\n",
    "                        continue\n",
    "\n",
    "                    # Prevent page number duplication\n",
    "                    h = _hash_text(text)\n",
    "                    if h in seen_hashes:\n",
    "                        continue\n",
    "                    seen_hashes.add(h)\n",
    "\n",
    "                    texts.append(text)\n",
    "\n",
    "                    # Select tokenizer (token/letter)\n",
    "                    if TOKEN_SPLIT_AVAILABLE:\n",
    "                        splitter = TokenTextSplitter(\n",
    "                            chunk_size=chunk_size_tokens,\n",
    "                            chunk_overlap=chunk_overlap_tokens,\n",
    "                            encoding_name=\"cl100k_base\"\n",
    "                        )\n",
    "                        page_chunks = splitter.split_text(text)\n",
    "                    else:\n",
    "                        splitter = RecursiveCharacterTextSplitter(\n",
    "                            chunk_size=800,\n",
    "                            chunk_overlap=100\n",
    "                        )\n",
    "                        page_chunks = splitter.split_text(text)\n",
    "\n",
    "                    # metadata saved to each chunk\n",
    "                    for ci, ch in enumerate(page_chunks):\n",
    "                        chunks.append({\n",
    "                            \"text\": ch,\n",
    "                            \"source\": file.name,\n",
    "                            \"page\": i,\n",
    "                            \"chunk_index\": ci\n",
    "                        })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load PDF: {file.name} ({e})\")\n",
    "\n",
    "    if not chunks:\n",
    "        print('No valid PDF chunks found in \"docs\" directory.')\n",
    "        return state\n",
    "\n",
    "    print(f\"Loaded {len(texts)} pages and created {len(chunks)} chunks.\")\n",
    "    state.docs = texts\n",
    "    state.chunks = chunks\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ddcf178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU to run if possible\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db3b8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Embedding models and load models to memory\n",
    "QWEN_MODEL = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "BGE_MODEL = \"BAAI/bge-m3\"\n",
    "qwen = SentenceTransformer(QWEN_MODEL, device=DEVICE)\n",
    "bge  = SentenceTransformer(BGE_MODEL,  device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4260ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vector dimension of each model (for qdrant collection)\n",
    "QWEN_DIM = qwen.get_sentence_embedding_dimension()\n",
    "BGE_DIM  = bge.get_sentence_embedding_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02bd476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant server connection configuration (localhost:6333)\n",
    "QDRANT_HOST = os.getenv(\"QDRANT_HOST\", \"localhost\")  # use default if no environment variables\n",
    "QDRANT_PORT = int(os.getenv(\"QDRANT_PORT\", \"6333\"))\n",
    "COLLECTION  = \"pkyoo_personal_docs_dualvec\"                # vector collection name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a66d1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant client reset and connection test\n",
    "try:\n",
    "    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT, timeout=60)\n",
    "    _ = client.get_collections()  # get collection lists to check connection\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect Qdrant. Check Qdrant Docker is running.\")\n",
    "    print(\"e.g., docker run -p 6333:6333 -v $(pwd)/qdrant_storage:/qdrant/storage qdrant/qdrant\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31de547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing = [c.name for c in client.get_collections().collections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "394de68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹCollection exists: pkyoo_personal_docs_dualvec\n"
     ]
    }
   ],
   "source": [
    "# Check collection existence and create collections if not\n",
    "if COLLECTION not in existing:\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION,\n",
    "        vectors_config={\n",
    "            \"qwen\": qmodels.VectorParams(size=QWEN_DIM, distance=qmodels.Distance.COSINE),\n",
    "            \"bge\":  qmodels.VectorParams(size=BGE_DIM,  distance=qmodels.Distance.COSINE)\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created collection: {COLLECTION}\")\n",
    "else:\n",
    "    print(f\"ℹCollection exists: {COLLECTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7791a21",
   "metadata": {},
   "source": [
    "Check http://localhost:6333/dashboard for local qdrant collections dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e8fd352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Embedding model for GPU memories. bge will still be used retrieval.\n",
    "# del qwen\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ced80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_store(state: RAGState, batch_size=128, upsert_batch=2048) -> RAGState:\n",
    "    if not state.chunks:\n",
    "        print(\"No chunks found. Run load_and_chunk() first.\")\n",
    "        return state\n",
    "\n",
    "    # Embedding only the text from chunk (No metadata)\n",
    "    texts_only = [c[\"text\"] for c in state.chunks]\n",
    "\n",
    "    print(\"Encoding with Qwen3-Embedding-0.6B ...\")\n",
    "    qwen_vecs = qwen.encode(\n",
    "        texts_only, batch_size=batch_size,\n",
    "        show_progress_bar=True, normalize_embeddings=True\n",
    "    )\n",
    "    print(\"Encoding with bge-m3 ...\")\n",
    "    bge_vecs = bge.encode(\n",
    "        texts_only, batch_size=batch_size,\n",
    "        show_progress_bar=True, normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    # Points (Use metadate from chunk dict)\n",
    "    points = []\n",
    "    for i, (qv, bv) in enumerate(zip(qwen_vecs, bge_vecs)):\n",
    "        ch = state.chunks[i]\n",
    "        payload = {\n",
    "            \"text\": ch[\"text\"],               # Only text saved in payload\n",
    "            \"chunk_index\": ch[\"chunk_index\"],\n",
    "            \"source\": ch.get(\"source\"),\n",
    "            \"page\": ch.get(\"page\"),\n",
    "        }\n",
    "        points.append(qmodels.PointStruct(\n",
    "            id=str(uuid.uuid4()),\n",
    "            vector={\"qwen\": qv.tolist(), \"bge\": bv.tolist()},\n",
    "            payload=payload\n",
    "        ))\n",
    "\n",
    "    print(f\"Upserting {len(points)} vectors → {COLLECTION}\")\n",
    "    for s in range(0, len(points), upsert_batch):\n",
    "        client.upsert(collection_name=COLLECTION, points=points[s:s+upsert_batch], wait=True)\n",
    "    print(\"Upsert finished.\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_qdrant(state: RAGState, top_k: int = 5) -> RAGState:\n",
    "    # Retrieves top-k most relevant document chunks from Qdrant\n",
    "    # based on the query embedding generated by bge-m3 (RTEB model).\n",
    "\n",
    "    if not state.query:\n",
    "        print(\"No user query provided in state.query\")\n",
    "        return state\n",
    "\n",
    "    print(\"Generating query embedding using bge-m3\")\n",
    "\n",
    "    # Step 1: Encode the query text using the retrieval embedding model (bge-m3)\n",
    "    query_vec = bge.encode(\n",
    "        [state.query],\n",
    "        normalize_embeddings=True  # cosine similarity requires normalized vectors\n",
    "    )[0]\n",
    "\n",
    "    # Step 2: Search in Qdrant using the 'bge' vector field\n",
    "    # The 'vector' argument must match the name used during embedding\n",
    "    print(f\"Searching Qdrant collection '{COLLECTION}' ...\")\n",
    "    hits = client.query_points(\n",
    "        collection_name=COLLECTION,\n",
    "        query_vector=(\"bge\", query_vec),\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    # Step 3: Extract the retrieved texts (payloads), Store text, metadata, score\n",
    "    results = []\n",
    "    for h in hits[:top_k]:\n",
    "        raw_text = h.payload.get(\"text\", \"\")\n",
    "        # Prevent the non-text dict saved just in case\n",
    "        if isinstance(raw_text, dict):\n",
    "            raw_text = raw_text.get(\"text\", \"\")\n",
    "        results.append({\n",
    "            \"text\": raw_text,\n",
    "            \"score\": getattr(h, \"score\", None),\n",
    "            \"id\": getattr(h, \"id\", None),\n",
    "            \"source\": h.payload.get(\"source\"),\n",
    "            \"page\": h.payload.get(\"page\"),\n",
    "            \"chunk_index\": h.payload.get(\"chunk_index\")\n",
    "        })\n",
    "\n",
    "    # Step 4: Store retrieved chunks in RAG state\n",
    "    state.results = results\n",
    "    print(f\"Retrieved {len(results)} chunks.\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15fc3f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "947b7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_ranked(state: RAGState, top_k: int = 5,\n",
    "                              relevance_threshold: float = 0.4,\n",
    "                              ndcg_threshold: float = 0.6,\n",
    "                              mrr_threshold: float = 0.5,\n",
    "                              use_qdrant_scores: bool = False) -> str:\n",
    "    # Evaluate retrieval quality using ranking metrics (nDCG@k, MRR).\n",
    "    # Returns 'generate' if the retrieval quality is good enough,\n",
    "    # otherwise 'rewrite' to trigger a query refinement step.\n",
    "\n",
    "    if not state.results:\n",
    "        print(\"No retrieved chunks to evaluate. Query rewrite required.\")\n",
    "        return \"rewrite\"\n",
    "\n",
    "    print(\"Evaluating retrieval quality using ranking metrics...\")\n",
    "    \n",
    "    # Evaluate the top_k retrieved texts\n",
    "    results_k = state.results[:top_k]\n",
    "    texts = [r[\"text\"] for r in results_k]\n",
    "\n",
    "    # Step 1: Encode the query vector using RTEB model (bge-m3)\n",
    "    query_vec = bge.encode([state.query], normalize_embeddings=True)[0]\n",
    "\n",
    "    # Step 2: Encode the retrieved chunks\n",
    "    retrieved_vecs = bge.encode(texts, normalize_embeddings=True)\n",
    "\n",
    "    # Step 3: Compute cosine similarity for each chunk\n",
    "    sims = np.dot(retrieved_vecs, query_vec)\n",
    "\n",
    "    # Step 4: Derive relevance labels (1 if above threshold, else 0)\n",
    "    relevance = (sims >= relevance_threshold).astype(int)\n",
    "\n",
    "    # Step 5: Compute ranking metrics (scores-predicted value vs relevance)\n",
    "    preds = np.array([r[\"score\"] for r in results_k]) if use_qdrant_scores else sims\n",
    "    ndcg = ndcg_score([relevance], [preds])\n",
    "    \n",
    "    if np.any(relevance == 1):\n",
    "        first_relevant_idx = int(np.argmax(relevance == 1))\n",
    "        reciprocal_rank = 1.0 / (first_relevant_idx + 1)\n",
    "    else:\n",
    "        reciprocal_rank = 0.0\n",
    "\n",
    "    print(f\"nDCG@{top_k}: {ndcg:.3f}, MRR: {reciprocal_rank:.3f}\")\n",
    "\n",
    "    # Step 6: Decision logic\n",
    "    if ndcg >= ndcg_threshold or reciprocal_rank >= mrr_threshold:\n",
    "        print(\"Retrieval ranking is satisfactory. Proceeding to generation.\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"Retrieval ranking is poor. Triggering query rewrite.\")\n",
    "        return \"rewrite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48dbfd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(state: RAGState) -> RAGState:\n",
    "    # Use Llama3 to rephrase the query semantically while keeping intent.\n",
    "    prompt = f\"\"\"\n",
    "    You are a query rewriter for a retrieval system.\n",
    "    Rephrase the following query to improve retrieval quality\n",
    "    without changing its meaning or intent.\n",
    "\n",
    "    Query:\n",
    "    \"{state.query}\"\n",
    "    \"\"\"\n",
    "\n",
    "    new_query = llama3.generate(prompt)  # pseudo-call\n",
    "    new_query = (new_query or \"\").strip()\n",
    "\n",
    "    print(f\"\"\"\n",
    "          Rewritten query (attempt {state.retry_count + 1}):\n",
    "          \"{new_query}\"\n",
    "          \"\"\")\n",
    "    state.query = new_query\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e74c7",
   "metadata": {},
   "source": [
    "Loop stops when number of rewrites = 5 to limit the response time and endless querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec15f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_loop(state: RAGState, max_retries: int = 5, top_k: int = 5) -> RAGState:\n",
    "    # Full retrieval + evaluation + rewrite loop to prevent hallucination.\n",
    "    while state.retry_count <= max_retries:\n",
    "        print(f\"\\n[Attempt {state.retry_count + 1}] Retrieving and evaluating...\")\n",
    "        \n",
    "        # Step 1: Retrieve\n",
    "        state = retrieve_from_qdrant(state, top_k=top_k)\n",
    "\n",
    "        # Step 2: Evaluate\n",
    "        result = evaluate_retrieval_ranked(state, top_k=top_k)\n",
    "\n",
    "        if result == \"generate\":\n",
    "            print(\"Retrieval sufficient to proceed LLM generation.\")\n",
    "            return state  # Pass to generation stage\n",
    "\n",
    "        # Step 3: If evaluation fails, rewrite query\n",
    "        if state.retry_count < max_retries:\n",
    "            print(\"Rewriting query and retrying retrieval...\")\n",
    "            state = rewrite_query(state)\n",
    "            state.retry_count += 1\n",
    "        else:\n",
    "            print(\"Retrieval failed after max attempts. No relevant info found.\")\n",
    "            state.answer = \"I'm sorry, but I couldn’t find relevant information about Paul from the database.\"\n",
    "            return state\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f56e1e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24046/309009186.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llama3 = Ollama(model=\"llama3\", temperature=0.2)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import textwrap\n",
    "llama3 = Ollama(model=\"llama3\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e46b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state: RAGState, model: str = \"llama3\", max_context: int = 5) -> RAGState:\n",
    "    \"\"\"\n",
    "    Generate an answer using retrieved chunks as context.\n",
    "    Uses a local Llama3 model (via Ollama) for answer generation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Retrieve top-k context chunks (from previous retrieval step)\n",
    "    if not state.results:\n",
    "        print(\"No retrieval results found. Run retrieval_loop() first.\")\n",
    "        state.answer = \"I'm sorry, but I couldn’t find relevant information about Paul from the database.\"\n",
    "        return state\n",
    "\n",
    "    # Limit context to top N results for efficiency\n",
    "    # Prevent dict that is not text\n",
    "    top_contexts_raw = [r.get(\"text\", \"\") for r in state.results[:max_context]]\n",
    "    top_contexts = [\n",
    "        (c.get(\"text\", \"\") if isinstance(c, dict) else c) for c in top_contexts_raw\n",
    "    ]\n",
    "    combined_context = \"\\n\\n\".join(top_contexts)\n",
    "\n",
    "    # Step 2: Build a system + user prompt\n",
    "    prompt = textwrap.dedent(f\"\"\"\n",
    "    You are a precise and concise AI assistant specialized in retrieval-augmented generation.\n",
    "    Use the following context extracted from trusted documents to answer the user's query accurately.\n",
    "    If the context does not contain sufficient information, clearly say so without hallucinating.\n",
    "\n",
    "    --- Context ---\n",
    "    {combined_context}\n",
    "\n",
    "    --- User Query ---\n",
    "    {state.query}\n",
    "\n",
    "    --- Instruction ---\n",
    "    1. Base your answer only on the given context.\n",
    "    2. DO NOT invent facts not present in the documents(context).\n",
    "    3. If unsure, say \"I'm sorry, but I couldn’t find relevant information about Paul from the database.\"\n",
    "    4. Return your final answer clearly and concisely.\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Generating answer...\")\n",
    "    try:\n",
    "        response = model.invoke(prompt)\n",
    "        answer_text = response.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect GenAI Model: {e}\")\n",
    "        answer_text = \"Error: failed to generate answer due to model connection or runtime issue.\"\n",
    "\n",
    "    # Step 3: Store generated answer\n",
    "    state.answer = answer_text\n",
    "\n",
    "    # Step 4: Log summary\n",
    "    print(\" \")\n",
    "    print(\"\\n====== Generated Answer ======\")\n",
    "    print(\" \")\n",
    "    print(answer_text)\n",
    "    print(\" \")\n",
    "    print(\"================================\\n\")\n",
    "    print(\" \")\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9c7fd",
   "metadata": {},
   "source": [
    "### GenAI generated Answer Evaluation\n",
    "1. Sementic Evaluation using DeepEval Faithfulness and Answer Relevancy\n",
    "  - Faithfulness: Is answer based on the context retrieved by the retrieval model?\n",
    "  - Answer Relevancy: Is answer relevant to the user query(question)?\n",
    "\n",
    "2. Liguistic Evaluation using DeepEval and custom metrics\n",
    "  - Grammar: Grammar Evaluation\n",
    "  - Fluency: Fluency Evaluation\n",
    "  - Coherence: Evaluating consistency of logical structure between sentences\n",
    "  - Conciseness: Evaluating the unnecessary repeatition\n",
    "  - Toxicity and Bias: Ethical use of words and sentences Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "345dca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models import OllamaModel\n",
    "from deepeval.metrics import FaithfulnessMetric, AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c939f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = OllamaModel(model=\"llama3\")\n",
    "\n",
    "faith_metric = FaithfulnessMetric(model=eval_model, threshold=0.7)\n",
    "relev_metric = AnswerRelevancyMetric(model=eval_model, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a71d2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer_tier1(state: RAGState,\n",
    "                          faith_thresh: float = 0.7,\n",
    "                          relev_thresh: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Tier 1 evaluation:\n",
    "    - Checks semantic validity (Faithfulness + Answer Relevancy)\n",
    "    - Returns 'pass' or 'rewrite'\n",
    "    \"\"\"\n",
    "\n",
    "    if not state.answer or not state.results:\n",
    "        print(\"No generated answer or context found for Tier 1 evaluation.\")\n",
    "        return \"rewrite\"\n",
    "\n",
    "    context = \"\\n\".join([r[\"text\"] for r in state.results])\n",
    "\n",
    "    # Define evaluation test case\n",
    "    test_case = LLMTestCase(\n",
    "        input=state.query,\n",
    "        actual_output=state.answer,\n",
    "        retrieval_context=[context],\n",
    "        expected_output=None\n",
    "    )\n",
    "\n",
    "    print(\"Evaluating Tier 1 - Faithfulness & Relevancy...\")\n",
    "    results = evaluate(test_cases=[test_case], metrics=[faith_metric, relev_metric])\n",
    "\n",
    "    # Extract metric scores\n",
    "    faith_score = results[0][faith_metric.name][\"score\"]\n",
    "    relev_score = results[0][relev_metric.name][\"score\"]\n",
    "\n",
    "    print(f\"Faithfulness: {faith_score:.3f} | Relevancy: {relev_score:.3f}\")\n",
    "\n",
    "    if faith_score >= faith_thresh and relev_score >= relev_thresh:\n",
    "        print(\"Passed Tier 1 - Semantic Evaluation\")\n",
    "        return \"pass\"\n",
    "    else:\n",
    "        print(\"Failed Tier 1 - Sending back to rewrite.\")\n",
    "        return \"rewrite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fdd72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import BaseMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e158c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarMetric(BaseMetric):\n",
    "    def __init__(self, model=\"llama3\", threshold=0.75):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "        self.name = \"Grammar Quality\"\n",
    "\n",
    "    def measure(self, question, answer, context=None):\n",
    "        prompt = f\"\"\"\n",
    "        Evaluate the grammar and sentence correctness of this answer.\n",
    "        Give a score between 0 (poor) and 1 (perfect grammar).\n",
    "        Answer:\n",
    "        {answer}\n",
    "        \"\"\"\n",
    "        score = self.call_model(prompt)\n",
    "        return float(score)\n",
    "\n",
    "class FluencyMetric(BaseMetric):\n",
    "    def __init__(self, model=\"llama3\", threshold=0.75):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "        self.name = \"Fluency\"\n",
    "\n",
    "    def measure(self, question, answer, context=None):\n",
    "        prompt = f\"\"\"\n",
    "        Rate the fluency and naturalness of this answer from 0 to 1.\n",
    "        Avoid judging content, only evaluate smoothness and readability.\n",
    "        Answer:\n",
    "        {answer}\n",
    "        \"\"\"\n",
    "        score = self.call_model(prompt)\n",
    "        return float(score)\n",
    "\n",
    "class CoherenceMetric(BaseMetric):\n",
    "    def __init__(self, model=\"llama3\", threshold=0.75):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "        self.name = \"Coherence\"\n",
    "\n",
    "    def measure(self, question, answer, context=None):\n",
    "        prompt = f\"\"\"\n",
    "        Evaluate the logical coherence and flow of the following answer.\n",
    "        Give a score from 0 (disjointed) to 1 (highly coherent).\n",
    "        Answer:\n",
    "        {answer}\n",
    "        \"\"\"\n",
    "        score = self.call_model(prompt)\n",
    "        return float(score)\n",
    "\n",
    "class ConcisenessMetric(BaseMetric):\n",
    "    def __init__(self, model=\"llama3\", threshold=0.6):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "        self.name = \"Conciseness\"\n",
    "\n",
    "    def measure(self, question, answer, context=None):\n",
    "        prompt = f\"\"\"\n",
    "        Evaluate how concise this answer is.\n",
    "        Penalize redundancy or overly long explanations.\n",
    "        Score from 0 (verbose) to 1 (clear and concise).\n",
    "        Answer:\n",
    "        {answer}\n",
    "        \"\"\"\n",
    "        score = self.call_model(prompt)\n",
    "        return float(score)\n",
    "\n",
    "class ToxicityMetric(BaseMetric):\n",
    "    def __init__(self, model=\"llama3\", threshold=0.2):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "        self.name = \"Toxicity\"\n",
    "\n",
    "    def measure(self, question, answer, context=None):\n",
    "        prompt = f\"\"\"\n",
    "        Detect any offensive, hateful, or toxic language in the following answer.\n",
    "        Give a score from 0 (completely safe) to 1 (very toxic).\n",
    "        Answer:\n",
    "        {answer}\n",
    "        \"\"\"\n",
    "        score = self.call_model(prompt)\n",
    "        return float(score)\n",
    "\n",
    "class BiasMetric(BaseMetric):\n",
    "    def __init__(self, model=\"llama3\", threshold=0.3):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "        self.name = \"Bias\"\n",
    "\n",
    "    def measure(self, question, answer, context=None):\n",
    "        prompt = f\"\"\"\n",
    "        Evaluate if this answer contains biased or discriminatory language.\n",
    "        Give a score from 0 (neutral) to 1 (biased).\n",
    "        Answer:\n",
    "        {answer}\n",
    "        \"\"\"\n",
    "        score = self.call_model(prompt)\n",
    "        return float(score)\n",
    "\n",
    "# --- Tier 2 Evaluation Logic ---\n",
    "def evaluate_answer_tier2(state: RAGState) -> str:\n",
    "    \"\"\"\n",
    "    Tier 2 evaluation:\n",
    "    - Checks Grammar, Fluency, Coherence, Conciseness, Toxicity, Bias\n",
    "    - Returns 'final' or 'rewrite_once'\n",
    "    \"\"\"\n",
    "\n",
    "    if not state.answer:\n",
    "        print(\"No answer found for Tier 2 evaluation.\")\n",
    "        return \"rewrite_once\"\n",
    "\n",
    "    print(\"Evaluating Tier 2 - linguistic/ethical evaluation...\")\n",
    "\n",
    "    metrics = [\n",
    "        GrammarMetric(), FluencyMetric(), CoherenceMetric(),\n",
    "        ConcisenessMetric(), ToxicityMetric(), BiasMetric()\n",
    "    ]\n",
    "\n",
    "    context = \"\\n\".join([r[\"text\"] for r in state.results]) if state.results else \"\"\n",
    "    question = state.query or \"General query\"\n",
    "\n",
    "    # Evaluate\n",
    "    scores = {}\n",
    "    for m in metrics:\n",
    "        try:\n",
    "            score = m.measure(question, state.answer, context)\n",
    "            scores[m.name] = score\n",
    "        except Exception as e:\n",
    "            scores[m.name] = 0.0\n",
    "            print(f\"Metric {m.name} failed: {e}\")\n",
    "\n",
    "    # Log scores\n",
    "    for k, v in scores.items():\n",
    "        print(f\"{k}: {v:.3f}\")\n",
    "\n",
    "    # Determine pass/fail\n",
    "    if (scores[\"Grammar Quality\"] >= 0.75 and\n",
    "        scores[\"Fluency\"] >= 0.75 and\n",
    "        scores[\"Coherence\"] >= 0.75 and\n",
    "        scores[\"Conciseness\"] >= 0.6 and\n",
    "        scores[\"Toxicity\"] <= 0.2 and\n",
    "        scores[\"Bias\"] <= 0.3):\n",
    "        print(\"Passed Tier 2 - Linguistic/Ethical Evaluation\")\n",
    "        return \"final\"\n",
    "    else:\n",
    "        print(\"Failed Tier 2 - Sending back to rewrite.\")\n",
    "        return \"rewrite_once\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b1ad224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_answer_evaluation(state: RAGState,\n",
    "                           max_semantic_retries: int = 5,\n",
    "                           max_linguistic_retries: int = 3) -> RAGState:\n",
    "    \"\"\"\n",
    "    Full evaluation pipeline:\n",
    "    - Tier 1 + Tier 2\n",
    "    - Adaptive retry: max total rewrites = 8\n",
    "    \"\"\"\n",
    "\n",
    "    # Tier 1 - Semantic evaluation\n",
    "    while state.retry_count < max_semantic_retries:\n",
    "        print(f\"\\n[Semantic Eval Attempt {state.retry_count + 1}]\")\n",
    "\n",
    "        tier1_result = evaluate_answer_tier1(state)\n",
    "        if tier1_result == \"pass\":\n",
    "            print(\"Passed Tier 1. Moving to Tier 2.\")\n",
    "            break\n",
    "\n",
    "        # Rewrite on fail\n",
    "        state = rewrite_answer(state)\n",
    "        state.retry_count += 1\n",
    "\n",
    "    else:\n",
    "        print(\"Tier 1 failed after max retries.\")\n",
    "        state.answer = (\n",
    "            \"I'm sorry, but I couldn’t generate a factually accurate and relevant answer. Could you try again?\"\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    # Tier 2 - Linguistic/Ethical evaluation\n",
    "    linguistic_retries = 0\n",
    "    while linguistic_retries < max_linguistic_retries:\n",
    "        print(f\"\\n[Linguistic Eval Attempt {linguistic_retries + 1}]\")\n",
    "\n",
    "        tier2_result = evaluate_answer_tier2(state)\n",
    "        if tier2_result == \"final\":\n",
    "            print(\"Passed Tier 2. Answer fully verified.\")\n",
    "            return state\n",
    "\n",
    "        # Rewrite & adjust retry counters\n",
    "        state = rewrite_answer(state)\n",
    "        linguistic_retries += 1\n",
    "\n",
    "        # Deduct semantic retry budget\n",
    "        if state.retry_count < max_semantic_retries:\n",
    "            state.retry_count += 1\n",
    "            print(f\"Deducted 1 semantic retry (used {state.retry_count}/{max_semantic_retries})\")\n",
    "\n",
    "        # Abort if total rewrite exceeds 8\n",
    "        if state.retry_count >= max_semantic_retries:\n",
    "            print(\"Answer generation total attempts exceeded 8. Aborting.\")\n",
    "            state.answer = (\n",
    "                \"I'm sorry, but I couldn’t refine the answer further without risking factual distortion. Could you try again?\"\n",
    "            )\n",
    "            return state\n",
    "\n",
    "    print(\"Minor linguistic issues remain. Accepting best-effort answer.\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3a111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading and Chunking Documents ---\n",
      "Loaded 21 pages and created 37 chunks.\n",
      "\n",
      "--- Embedding and Storing Chunks in Qdrant ---\n",
      "Encoding with Qwen3-Embedding-0.6B ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding with bge-m3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "/tmp/ipykernel_24046/2927138612.py:20: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting 37 vectors → pkyoo_personal_docs_dualvec\n",
      "Upsert finished.\n",
      "\n",
      "--- Starting Retrieval Loop ---\n",
      "\n",
      "[Attempt 1] Retrieving and evaluating...\n",
      "Generating query embedding using bge-m3\n",
      "Searching Qdrant collection 'pkyoo_personal_docs_dualvec' ...\n",
      "Retrieved 5 chunks.\n",
      "Evaluating retrieval quality using ranking metrics...\n",
      "nDCG@5: 1.000, MRR: 1.000\n",
      "Retrieval ranking is satisfactory. Proceeding to generation.\n",
      "Retrieval sufficient to proceed LLM generation.\n",
      "\n",
      "--- Generating Answer ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, dict found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Answer Generation using Llama3\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Generating Answer ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m state = \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Dual-layer Evaluation (DeepEval Tier1 + Custom Tier2)\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Evaluating Generated Answer ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mgenerate_answer\u001b[39m\u001b[34m(state, model, max_context)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Limit context to top N results for efficiency\u001b[39;00m\n\u001b[32m     14\u001b[39m top_contexts = [r[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m state.results[:max_context]]\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m combined_context = \u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_contexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Step 2: Build a system + user prompt\u001b[39;00m\n\u001b[32m     18\u001b[39m prompt = textwrap.dedent(\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[33mYou are a precise and concise AI assistant specialized in retrieval-augmented generation.\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[33mUse the following context extracted from trusted documents to answer the user\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms query accurately.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m \u001b[33m4. Return your final answer clearly and concisely.\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: sequence item 0: expected str instance, dict found"
     ]
    }
   ],
   "source": [
    "# Initialize State\n",
    "state = RAGState()\n",
    "state.query = input(\"Enter your question: \").strip()\n",
    "\n",
    "# Load & Chunk PDFs (only once unless documents change)\n",
    "print(\"\\n--- Loading and Chunking Documents ---\")\n",
    "state = load_and_chunk(state)\n",
    "\n",
    "# Embed and Store in Qdrant (only once after load)\n",
    "print(\"\\n--- Embedding and Storing Chunks in Qdrant ---\")\n",
    "state = embed_and_store(state)\n",
    "\n",
    "# release Qwen from GPU to free memory\n",
    "print(\"Deleting MTEB model from cache\")\n",
    "del qwen\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Retrieval Loop — uses bge-m3 for retrieval, evaluation & rewriting if needed\n",
    "print(\"\\n--- Starting Retrieval Loop ---\")\n",
    "state = retrieval_loop(state)\n",
    "\n",
    "# Answer Generation using Llama3\n",
    "print(\"\\n--- Generating Answer ---\")\n",
    "state = generate_answer(state)\n",
    "\n",
    "# Dual-layer Evaluation (DeepEval Tier1 + Custom Tier2)\n",
    "print(\"\\n--- Evaluating Generated Answer ---\")\n",
    "state = full_answer_evaluation(state)\n",
    "\n",
    "# Final Output (Answer to the question of user query)\n",
    "print(\"\\n=====================================================\")\n",
    "print(\"\")\n",
    "print(state.answer)\n",
    "print(\"\")\n",
    "print(\"=====================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ce1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasongraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
