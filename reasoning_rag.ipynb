{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec535d77",
   "metadata": {},
   "source": [
    "Activate venv and install packages\n",
    "1. !source reasongraph/bin/activate </br>\n",
    "2. !pip install \"langchain>=0.3\" \"langgraph>=0.2\" qdrant-client sentence-transformers torch pydantic python-dotenv\n",
    "3. !pip install pymupdf tiktoken # Count number of tokens to check model compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6510ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz, hashlib, torch, uuid, os, sys\n",
    "from langgraph.graph import StateGraph, END\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Dict, List, Optional\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGState(BaseModel):\n",
    "    docs: List[str] = Field(default_factory=list)\n",
    "    chunks: List[Dict[str, Any]] = Field(default_factory=list) # Includes metadata\n",
    "    query: Optional[str] = None\n",
    "    results: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "    answer: Optional[str] = None\n",
    "    retry_count: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a899e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    TOKEN_SPLIT_AVAILABLE = True\n",
    "except Exception:\n",
    "    TOKEN_SPLIT_AVAILABLE = False\n",
    "\n",
    "def _hash_text(t: str) -> str:\n",
    "    \"\"\"페이지 중복 제거용 해시 함수\"\"\"\n",
    "    return hashlib.sha256(t.strip().encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def load_and_chunk(state: RAGState, folder: str = \"docs\",\n",
    "                   chunk_size_tokens: int = 350,\n",
    "                   chunk_overlap_tokens: int = 50) -> RAGState:\n",
    "    texts, chunks, seen_hashes = [], [], set()\n",
    "\n",
    "    for file in Path(folder).rglob(\"*.pdf\"):\n",
    "        try:\n",
    "            with fitz.open(file) as pdf:\n",
    "                for i, page in enumerate(pdf, start=1):\n",
    "                    text = page.get_text(\"text\")\n",
    "                    if not text or not text.strip():\n",
    "                        continue\n",
    "\n",
    "                    # Prevent page number duplication\n",
    "                    h = _hash_text(text)\n",
    "                    if h in seen_hashes:\n",
    "                        continue\n",
    "                    seen_hashes.add(h)\n",
    "\n",
    "                    texts.append(text)\n",
    "\n",
    "                    # Select tokenizer (token/letter)\n",
    "                    if TOKEN_SPLIT_AVAILABLE:\n",
    "                        splitter = TokenTextSplitter(\n",
    "                            chunk_size=chunk_size_tokens,\n",
    "                            chunk_overlap=chunk_overlap_tokens,\n",
    "                            encoding_name=\"cl100k_base\"\n",
    "                        )\n",
    "                        page_chunks = splitter.split_text(text)\n",
    "                    else:\n",
    "                        splitter = RecursiveCharacterTextSplitter(\n",
    "                            chunk_size=800,\n",
    "                            chunk_overlap=100\n",
    "                        )\n",
    "                        page_chunks = splitter.split_text(text)\n",
    "\n",
    "                    # metadata saved to each chunk\n",
    "                    for ci, ch in enumerate(page_chunks):\n",
    "                        chunks.append({\n",
    "                            \"text\": ch,\n",
    "                            \"source\": file.name,\n",
    "                            \"page\": i,\n",
    "                            \"chunk_index\": ci\n",
    "                        })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load PDF: {file.name} ({e})\")\n",
    "\n",
    "    if not chunks:\n",
    "        print('No valid PDF chunks found in \"docs\" directory.')\n",
    "        return state\n",
    "\n",
    "    print(f\"Loaded {len(texts)} pages and created {len(chunks)} chunks.\")\n",
    "    state.docs = texts\n",
    "    state.chunks = chunks\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcf178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU to run if possible\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Embedding models and load models to memory\n",
    "QWEN_MODEL = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "BGE_MODEL = \"BAAI/bge-m3\"\n",
    "qwen = SentenceTransformer(QWEN_MODEL, device=DEVICE)\n",
    "bge  = SentenceTransformer(BGE_MODEL,  device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4260ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vector dimension of each model (for qdrant collection)\n",
    "QWEN_DIM = qwen.get_sentence_embedding_dimension()\n",
    "BGE_DIM  = bge.get_sentence_embedding_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant server connection configuration (localhost:6333)\n",
    "QDRANT_HOST = os.getenv(\"QDRANT_HOST\", \"localhost\")  # use default if no environment variables\n",
    "QDRANT_PORT = int(os.getenv(\"QDRANT_PORT\", \"6333\"))\n",
    "COLLECTION  = \"pkyoo_personal_docs_dualvec\"                # vector collection name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant client reset and connection test\n",
    "try:\n",
    "    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT, timeout=60)\n",
    "    _ = client.get_collections()  # get collection lists to check connection\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect Qdrant. Check Qdrant Docker is running.\")\n",
    "    print(\"e.g., docker run -p 6333:6333 -v $(pwd)/qdrant_storage:/qdrant/storage qdrant/qdrant\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing = [c.name for c in client.get_collections().collections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394de68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check collection existence and create collections if not\n",
    "if COLLECTION not in existing:\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION,\n",
    "        vectors_config={\n",
    "            \"qwen\": qmodels.VectorParams(size=QWEN_DIM, distance=qmodels.Distance.COSINE),\n",
    "            \"bge\":  qmodels.VectorParams(size=BGE_DIM,  distance=qmodels.Distance.COSINE)\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created collection: {COLLECTION}\")\n",
    "else:\n",
    "    print(f\"ℹCollection exists: {COLLECTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7791a21",
   "metadata": {},
   "source": [
    "Check http://localhost:6333/dashboard for local qdrant collections dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8fd352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Embedding model for GPU memories. bge will still be used retrieval.\n",
    "# del qwen\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ced80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_store(state: RAGState, batch_size=128, upsert_batch=2048) -> RAGState:\n",
    "    if not state.chunks:\n",
    "        print(\"No chunks found. Run load_and_chunk() first.\")\n",
    "        return state\n",
    "\n",
    "    # 1) 임베딩\n",
    "    print(\"Encoding with Qwen3-Embedding-0.6B ...\")\n",
    "    qwen_vecs = qwen.encode(\n",
    "        state.chunks, batch_size=batch_size,\n",
    "        show_progress_bar=True, normalize_embeddings=True\n",
    "        )\n",
    "    print(\"Encoding with bge-m3 ...\")\n",
    "    bge_vecs = bge.encode(\n",
    "        state.chunks, batch_size=batch_size,\n",
    "        show_progress_bar=True, normalize_embeddings=True\n",
    "        )\n",
    "\n",
    "    # 2) 포인트 준비 (메타 포함)\n",
    "    points = []\n",
    "    for i, (qv, bv) in enumerate(zip(qwen_vecs, bge_vecs)):\n",
    "        meta = getattr(state, \"_chunk_meta\", None)\n",
    "        source = meta[i][\"source\"] if meta else None\n",
    "        page = meta[i][\"page\"] if meta else None\n",
    "\n",
    "        payload = {\n",
    "            \"text\": state.chunks[i],\n",
    "            \"chunk_index\": i,\n",
    "            \"source\": source,\n",
    "            \"page\": page,\n",
    "        }\n",
    "        points.append(qmodels.PointStruct(\n",
    "            id=str(uuid.uuid4()),\n",
    "            vector={\"qwen\": qv.tolist(), \"bge\": bv.tolist()},\n",
    "            payload=payload\n",
    "        ))\n",
    "\n",
    "    # 3) 배치 업서트\n",
    "    print(f\"Upserting {len(points)} vectors → {COLLECTION}\")\n",
    "    for s in range(0, len(points), upsert_batch):\n",
    "        client.upsert(collection_name=COLLECTION, points=points[s:s+upsert_batch], wait=True)\n",
    "    print(\"Upsert finished.\")\n",
    "    return state\n",
    "\n",
    "# 임베딩이 끝난 후에만 qwen을 비움\n",
    "# del qwen; torch.cuda.empty_cache() 는 embed_and_store 호출 이후 실행할 것.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_qdrant(state: RAGState, top_k: int = 5) -> RAGState:\n",
    "    # Retrieves top-k most relevant document chunks from Qdrant\n",
    "    # based on the query embedding generated by bge-m3 (RTEB model).\n",
    "\n",
    "    if not state.query:\n",
    "        print(\"No user query provided in state.query\")\n",
    "        return state\n",
    "\n",
    "    print(\"Generating query embedding using bge-m3\")\n",
    "\n",
    "    # Step 1: Encode the query text using the retrieval embedding model (bge-m3)\n",
    "    query_vec = bge.encode(\n",
    "        [state.query],\n",
    "        normalize_embeddings=True  # cosine similarity requires normalized vectors\n",
    "    )[0]\n",
    "\n",
    "    # Step 2: Search in Qdrant using the 'bge' vector field\n",
    "    # The 'vector' argument must match the name used during embedding\n",
    "    print(f\"Searching Qdrant collection '{COLLECTION}' ...\")\n",
    "    hits = client.search(\n",
    "        collection_name=COLLECTION,\n",
    "        query_vector=(\"bge\", query_vec),\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    # Step 3: Extract the retrieved texts (payloads), Store text, metadata, score\n",
    "    results = []\n",
    "    for h in hits[:top_k]:\n",
    "        results.append({\n",
    "            \"text\": h.payload.get(\"text\", \"\"),\n",
    "            \"score\": getattr(h, \"score\", None),\n",
    "            \"id\": getattr(h, \"id\", None),\n",
    "            \"source\": h.payload.get(\"source\"),\n",
    "            \"page\": h.payload.get(\"page\"),\n",
    "            \"chunk_index\": h.payload.get(\"chunk_index\")\n",
    "        })\n",
    "\n",
    "    # Step 4: Store retrieved chunks in RAG state\n",
    "    state.results = results\n",
    "    print(f\"Retrieved {len(results)} chunks.\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc3f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_ranked(state: RAGState, top_k: int = 5,\n",
    "                              relevance_threshold: float = 0.4,\n",
    "                              ndcg_threshold: float = 0.6,\n",
    "                              mrr_threshold: float = 0.5,\n",
    "                              use_qdrant_scores: bool = False) -> str:\n",
    "    # Evaluate retrieval quality using ranking metrics (nDCG@k, MRR).\n",
    "    # Returns 'generate' if the retrieval quality is good enough,\n",
    "    # otherwise 'rewrite' to trigger a query refinement step.\n",
    "\n",
    "    if not state.results:\n",
    "        print(\"No retrieved chunks to evaluate. Query rewrite required.\")\n",
    "        return \"rewrite\"\n",
    "\n",
    "    print(\"Evaluating retrieval quality using ranking metrics...\")\n",
    "    \n",
    "    # Evaluate the top_k retrieved texts\n",
    "    results_k = state.results[:top_k]\n",
    "    texts = [r[\"text\"] for r in results_k]\n",
    "\n",
    "    # Step 1: Encode the query vector using RTEB model (bge-m3)\n",
    "    query_vec = bge.encode([state.query], normalize_embeddings=True)[0]\n",
    "\n",
    "    # Step 2: Encode the retrieved chunks\n",
    "    retrieved_vecs = bge.encode(texts, normalize_embeddings=True)\n",
    "\n",
    "    # Step 3: Compute cosine similarity for each chunk\n",
    "    sims = np.dot(retrieved_vecs, query_vec)\n",
    "\n",
    "    # Step 4: Derive relevance labels (1 if above threshold, else 0)\n",
    "    relevance = (sims >= relevance_threshold).astype(int)\n",
    "\n",
    "    # Step 5: Compute ranking metrics (scores-predicted value vs relevance)\n",
    "    preds = np.array([r[\"score\"] for r in results_k]) if use_qdrant_scores else sims\n",
    "    ndcg = ndcg_score([relevance], [preds])\n",
    "    \n",
    "    if np.any(relevance == 1):\n",
    "        first_relevant_idx = int(np.argmax(relevance == 1))\n",
    "        reciprocal_rank = 1.0 / (first_relevant_idx + 1)\n",
    "    else:\n",
    "        reciprocal_rank = 0.0\n",
    "\n",
    "    print(f\"nDCG@{top_k}: {ndcg:.3f}, MRR: {reciprocal_rank:.3f}\")\n",
    "\n",
    "    # Step 6: Decision logic\n",
    "    if ndcg >= ndcg_threshold or reciprocal_rank >= mrr_threshold:\n",
    "        print(\"Retrieval ranking is satisfactory. Proceeding to generation.\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"Retrieval ranking is poor. Triggering query rewrite.\")\n",
    "        return \"rewrite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dbfd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(state: RAGState) -> RAGState:\n",
    "    # Use Llama3 to rephrase the query semantically while keeping intent.\n",
    "    prompt = f\"\"\"\n",
    "    You are a query rewriter for a retrieval system.\n",
    "    Rephrase the following query to improve retrieval quality\n",
    "    without changing its meaning or intent.\n",
    "\n",
    "    Query:\n",
    "    \"{state.query}\"\n",
    "    \"\"\"\n",
    "\n",
    "    new_query = llama3.generate(prompt)  # pseudo-call\n",
    "    new_query = (new_query or \"\").strip()\n",
    "\n",
    "    print(f\"\"\"\n",
    "          Rewritten query (attempt {state.retry_count + 1}):\n",
    "          \"{new_query}\"\n",
    "          \"\"\")\n",
    "    state.query = new_query\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e74c7",
   "metadata": {},
   "source": [
    "Loop stops when number of rewrites = 5 to limit the response time and endless querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_loop(state: RAGState, max_retries: int = 5, top_k: int = 5) -> RAGState:\n",
    "    # Full retrieval + evaluation + rewrite loop to prevent hallucination.\n",
    "    while state.retry_count <= max_retries:\n",
    "        print(f\"\\n[Attempt {state.retry_count + 1}] Retrieving and evaluating...\")\n",
    "        \n",
    "        # Step 1: Retrieve\n",
    "        state = retrieve_from_qdrant(state, top_k=top_k)\n",
    "\n",
    "        # Step 2: Evaluate\n",
    "        result = evaluate_retrieval_ranked(state, top_k=top_k)\n",
    "\n",
    "        if result == \"generate\":\n",
    "            print(\"Retrieval sufficient to proceed LLM generation.\")\n",
    "            return state  # Pass to generation stage\n",
    "\n",
    "        # Step 3: If evaluation fails, rewrite query\n",
    "        if state.retry_count < max_retries:\n",
    "            print(\"Rewriting query and retrying retrieval...\")\n",
    "            state = rewrite_query(state)\n",
    "            state.retry_count += 1\n",
    "        else:\n",
    "            print(\"Retrieval failed after max attempts. No relevant info found.\")\n",
    "            state.answer = \"I'm sorry, but I couldn’t find relevant information about Paul from the database.\"\n",
    "            return state\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "llama3 = Ollama(model=\"llama3\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e46b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state: RAGState, model_name: str = \"llama3\", max_context: int = 5) -> RAGState:\n",
    "    \"\"\"\n",
    "    Generate an answer using retrieved chunks as context.\n",
    "    Uses a local Llama3 model (via Ollama) for answer generation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Retrieve top-k context chunks (from previous retrieval step)\n",
    "    if not state.results:\n",
    "        print(\"No retrieval results found. Run retrieval_loop() first.\")\n",
    "        state.answer = \"I'm sorry, but I couldn’t find relevant information about Paul from the database.\"\n",
    "        return state\n",
    "\n",
    "    # Limit context to top N results for efficiency\n",
    "    top_contexts = [r[\"text\"] for r in state.results[:max_context]]\n",
    "    combined_context = \"\\n\\n\".join(top_contexts)\n",
    "\n",
    "    # Step 2: Build a system + user prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a precise and concise AI assistant specialized in retrieval-augmented generation.\n",
    "    Use the following context extracted from trusted documents to answer the user's query accurately.\n",
    "    If the context does not contain sufficient information, clearly say so without hallucinating.\n",
    "\n",
    "    --- Context ---\n",
    "    {combined_context}\n",
    "\n",
    "    --- User Query ---\n",
    "    {state.query}\n",
    "\n",
    "    --- Instruction ---\n",
    "    1. Base your answer only on the given context.\n",
    "    2. DO NOT invent facts not present in the documents(context).\n",
    "    3. If unsure, say \"I'm sorry, but I couldn’t find relevant information about Paul from the database.\"\n",
    "    4. Return your final answer clearly and concisely.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Generating answer...\")\n",
    "    try:\n",
    "        response = model_name.invoke(prompt)\n",
    "        answer_text = response.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect GenAI Model: {e}\")\n",
    "        answer_text = \"Error: failed to generate answer due to model connection or runtime issue.\"\n",
    "\n",
    "    # Step 3: Store generated answer\n",
    "    state.answer = answer_text\n",
    "\n",
    "    # Step 4: Log summary\n",
    "    print(\"\\n====== Generated Answer ======\")\n",
    "    print(\" \")\n",
    "    print(answer_text)\n",
    "    print(\" \")\n",
    "    print(\"================================\\n\")\n",
    "\n",
    "    return state\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasongraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
