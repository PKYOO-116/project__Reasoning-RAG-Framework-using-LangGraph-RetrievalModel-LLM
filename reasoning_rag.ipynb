{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46efdfbf",
   "metadata": {},
   "source": [
    "# Basic Structuring for Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919baeb",
   "metadata": {},
   "source": [
    "## Install necessary LLM and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec535d77",
   "metadata": {},
   "source": [
    "Activate venv and install packages\n",
    "1. !source reasongraph/bin/activate </br>\n",
    "2. !pip install \"langchain>=0.3\" \"langgraph>=0.2\" qdrant-client sentence-transformers torch pydantic python-dotenv\n",
    "3. !pip install pymupdf tiktoken # Count number of tokens to check model compatibility\n",
    "\n",
    "Install ollama for answer generation and evaluation\n",
    "1. !curl -fsSL https://ollama.com/install.sh | sh\n",
    "2. !ollama pull llama3 # 8B Parameters, about 5GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6510ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz, hashlib, torch, uuid, os, re, textwrap\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pathlib import Path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "from langchain_community.llms import Ollama\n",
    "from deepeval.models import OllamaModel\n",
    "from deepeval.metrics import FaithfulnessMetric, AnswerRelevancyMetric, BaseMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64dfb9d",
   "metadata": {},
   "source": [
    "## RAG State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGState(BaseModel):\n",
    "    docs: List[str] = Field(default_factory=list)\n",
    "    chunks: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "    query: Optional[str] = None\n",
    "    results: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "    answer: Optional[str] = None\n",
    "    retry_count: int = 0\n",
    "    status: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7258dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Token Splitter Availability Check ---\n",
    "try:\n",
    "    _ = TokenTextSplitter\n",
    "    TOKEN_SPLIT_AVAILABLE = True\n",
    "except Exception:\n",
    "    TOKEN_SPLIT_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08432ad",
   "metadata": {},
   "source": [
    "## Document Loading and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee70ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hash_text(t: str) -> str:\n",
    "    return hashlib.sha256(t.strip().encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0adb391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_chunk(state: RAGState, folder: str = \"docs\",\n",
    "                   chunk_size_tokens: int = 350,\n",
    "                   chunk_overlap_tokens: int = 50) -> RAGState:\n",
    "    texts, chunks, seen_hashes = [], [], set()\n",
    "    print(\"\\n\\033[1;42m--- Start Loading Docs and Chunking ---\\033[0m\")\n",
    "\n",
    "    for file in Path(folder).rglob(\"*.pdf\"):\n",
    "        try:\n",
    "            with fitz.open(file) as pdf:\n",
    "                for i, page in enumerate(pdf, start=1):\n",
    "                    text = page.get_text(\"text\")\n",
    "                    if not text.strip():\n",
    "                        continue\n",
    "                    h = _hash_text(text)\n",
    "                    if h in seen_hashes: continue\n",
    "                    seen_hashes.add(h)\n",
    "                    texts.append(text)\n",
    "                    if TOKEN_SPLIT_AVAILABLE:\n",
    "                        splitter = TokenTextSplitter(\n",
    "                            chunk_size=chunk_size_tokens, chunk_overlap=chunk_overlap_tokens, encoding_name=\"cl100k_base\"\n",
    "                        )\n",
    "                        page_chunks = splitter.split_text(text)\n",
    "                    else:\n",
    "                        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "                        page_chunks = splitter.split_text(text)\n",
    "                    for ci, ch in enumerate(page_chunks):\n",
    "                        chunks.append({\"text\": ch, \"source\": file.name, \"page\": i, \"chunk_index\": ci})\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load PDF: {file.name} ({e})\")\n",
    "\n",
    "    if not chunks:\n",
    "        print('\\n\\033[1;42m--- No valid PDF chunks found in \"docs\" directory.---\\033[0m')\n",
    "        return state\n",
    "    print(f\"\\n\\033[1;42m---Loaded {len(texts)} pages and created {len(chunks)} chunks---\\033[0m\")\n",
    "    state.docs, state.chunks = texts, chunks\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_chunk(state: RAGState, folder: str = \"docs\",\n",
    "#                    chunk_size_tokens: int = 350,\n",
    "#                    chunk_overlap_tokens: int = 50) -> RAGState:\n",
    "#     texts, chunks, seen_hashes = [], [], set()\n",
    "\n",
    "#     print(\"\\n\\033[1;42m--- Start Loading Docs and Chunking ---\\033[0m\")\n",
    "\n",
    "#     for file in Path(folder).rglob(\"*.pdf\"):\n",
    "#         try:\n",
    "#             with fitz.open(file) as pdf:\n",
    "#                 for i, page in enumerate(pdf, start=1):\n",
    "#                     text = page.get_text(\"text\")\n",
    "#                     if not text.strip():\n",
    "#                         continue\n",
    "\n",
    "#                     # Prevent page number duplication\n",
    "#                     h = _hash_text(text)\n",
    "#                     if h in seen_hashes:\n",
    "#                         continue\n",
    "\n",
    "#                     seen_hashes.add(h)\n",
    "#                     texts.append(text)\n",
    "\n",
    "#                     # Select tokenizer (token/letter)\n",
    "#                     if TOKEN_SPLIT_AVAILABLE:\n",
    "#                         splitter = TokenTextSplitter(\n",
    "#                             chunk_size=chunk_size_tokens,\n",
    "#                             chunk_overlap=chunk_overlap_tokens,\n",
    "#                             encoding_name=\"cl100k_base\"\n",
    "#                         )\n",
    "#                         page_chunks = splitter.split_text(text)\n",
    "#                     else:\n",
    "#                         splitter = RecursiveCharacterTextSplitter(\n",
    "#                             chunk_size=800,\n",
    "#                             chunk_overlap=100\n",
    "#                         )\n",
    "#                         page_chunks = splitter.split_text(text)\n",
    "\n",
    "#                     # metadata saved to each chunk\n",
    "#                     for ci, ch in enumerate(page_chunks):\n",
    "#                         chunks.append({\n",
    "#                             \"text\": ch,\n",
    "#                             \"source\": file.name,\n",
    "#                             \"page\": i,\n",
    "#                             \"chunk_index\": ci\n",
    "#                         })\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to load PDF: {file.name} ({e})\")\n",
    "\n",
    "#     if not chunks:\n",
    "#         print('\\n\\033[1;42m--- No valid PDF chunks found in \"docs\" directory.---\\033[0m')\n",
    "#         return state\n",
    "\n",
    "#     print(\"\\n\\033[1;42m---Loaded {len(texts)} pages and created {len(chunks)} chunks---\\033[0m\")\n",
    "#     state.docs = texts\n",
    "#     state.chunks = chunks\n",
    "#     return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade03e8f",
   "metadata": {},
   "source": [
    "## Qdrant Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcf178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU to run if possible\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Embedding models and load models to memory\n",
    "QWEN_MODEL = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "BGE_MODEL = \"BAAI/bge-m3\"\n",
    "qwen = SentenceTransformer(QWEN_MODEL, device=DEVICE)\n",
    "bge  = SentenceTransformer(BGE_MODEL,  device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4260ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vector dimension of each model (for qdrant collection)\n",
    "QWEN_DIM = qwen.get_sentence_embedding_dimension()\n",
    "BGE_DIM  = bge.get_sentence_embedding_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant server connection configuration (localhost:6333)\n",
    "QDRANT_HOST = os.getenv(\"QDRANT_HOST\", \"localhost\")  # use default if no environment variables\n",
    "QDRANT_PORT = int(os.getenv(\"QDRANT_PORT\", \"6333\"))\n",
    "COLLECTION  = \"pkyoo_personal_docs_dualvec\"                # vector collection name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant client reset and connection test\n",
    "try:\n",
    "    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT, timeout=60)\n",
    "    _ = client.get_collections()  # get collection lists to check connection\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect Qdrant. Check Qdrant Docker is running.\")\n",
    "    print(\"e.g., docker run -p 6333:6333 -v $(pwd)/qdrant_storage:/qdrant/storage qdrant/qdrant\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing = [c.name for c in client.get_collections().collections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394de68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check collection existence and create collections if not\n",
    "if COLLECTION not in existing:\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION,\n",
    "        vectors_config={\n",
    "            \"qwen\": qmodels.VectorParams(size=QWEN_DIM, distance=qmodels.Distance.COSINE),\n",
    "            \"bge\":  qmodels.VectorParams(size=BGE_DIM,  distance=qmodels.Distance.COSINE)\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created collection: {COLLECTION}\")\n",
    "else:\n",
    "    print(f\"ℹCollection exists: {COLLECTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7791a21",
   "metadata": {},
   "source": [
    "Check http://localhost:6333/dashboard for local qdrant collections dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8fd352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Embedding model for GPU memories. bge will still be used retrieval.\n",
    "# del qwen\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ced80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_store(state: RAGState, batch_size=128, upsert_batch=2048) -> RAGState:\n",
    "    print(\"\\n\\033[1;42m--- Start Embedding and Storing ---\\033[0m\")\n",
    "\n",
    "    if not state.chunks:\n",
    "        print(\"No chunks found. Run load_and_chunk() first.\")\n",
    "        return state\n",
    "\n",
    "    # Embedding only the text from chunk (No metadata)\n",
    "    texts_only = [c[\"text\"] for c in state.chunks]\n",
    "\n",
    "    print(\"Encoding with Qwen3-Embedding-0.6B ...\")\n",
    "    qwen_vecs = qwen.encode(\n",
    "        texts_only, batch_size=batch_size,\n",
    "        show_progress_bar=True, normalize_embeddings=True\n",
    "    )\n",
    "    print(\"Encoding with bge-m3 ...\")\n",
    "    bge_vecs = bge.encode(\n",
    "        texts_only, batch_size=batch_size,\n",
    "        show_progress_bar=True, normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    # Points (Use metadate from chunk dict)\n",
    "    points = []\n",
    "    for i, (qv, bv) in enumerate(zip(qwen_vecs, bge_vecs)):\n",
    "        ch = state.chunks[i]\n",
    "        payload = {\n",
    "            \"text\": ch[\"text\"],               # Only text saved in payload\n",
    "            \"chunk_index\": ch[\"chunk_index\"],\n",
    "            \"source\": ch.get(\"source\"),\n",
    "            \"page\": ch.get(\"page\"),\n",
    "        }\n",
    "        points.append(qmodels.PointStruct(\n",
    "            id=str(uuid.uuid4()),\n",
    "            vector={\"qwen\": qv.tolist(), \"bge\": bv.tolist()},\n",
    "            payload=payload\n",
    "        ))\n",
    "\n",
    "    print(f\"Upserting {len(points)} vectors → {COLLECTION}\")\n",
    "    for s in range(0, len(points), upsert_batch):\n",
    "        client.upsert(collection_name=COLLECTION, points=points[s:s+upsert_batch], wait=True)\n",
    "    print(\"Upsert finished.\")\n",
    "    print(\"\\n\\033[1;42m--- End Embedding and Storing process ---\\033[0m\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbeaca7",
   "metadata": {},
   "source": [
    "## Retrieval from Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_qdrant(state: RAGState, top_k: int = 5) -> RAGState:\n",
    "    # Retrieves top-k most relevant document chunks from Qdrant\n",
    "    # based on the query embedding generated by bge-m3 (RTEB model).\n",
    "\n",
    "    print(\"\\n\\033[1;42m--- Start Retrieval from Qdrant Process ---\\033[0m\")\n",
    "\n",
    "    if not state.query:\n",
    "        print(\"No user query provided in state.query\")\n",
    "        return state\n",
    "\n",
    "    print(\"Generating query embedding using bge-m3\")\n",
    "\n",
    "    # Step 1: Encode the query text using the retrieval embedding model (bge-m3)\n",
    "    query_vec = bge.encode(\n",
    "        [state.query],\n",
    "        normalize_embeddings=True  # cosine similarity requires normalized vectors\n",
    "    )[0]\n",
    "\n",
    "    # Step 2: Search in Qdrant using the 'bge' vector field\n",
    "    # The 'vector' argument must match the name used during embedding\n",
    "    print(f\"Searching Qdrant collection '{COLLECTION}' ...\")\n",
    "    hits = client.query_points(\n",
    "        collection_name=COLLECTION,\n",
    "        query=query_vec,\n",
    "        using=\"bge\",\n",
    "        limit=top_k\n",
    "    ).points\n",
    "\n",
    "    # Step 3: Extract the retrieved texts (payloads), Store text, metadata, score\n",
    "    results = []\n",
    "    for h in hits:\n",
    "        raw_text = h.payload.get(\"text\", \"\")\n",
    "\n",
    "        # Prevent the non-text dict saved payloads just in case\n",
    "        if isinstance(raw_text, dict):\n",
    "            raw_text = raw_text.get(\"text\", \"\")\n",
    "            \n",
    "        results.append({\n",
    "            \"text\": raw_text,\n",
    "            \"score\": getattr(h, \"score\", None),\n",
    "            \"id\": getattr(h, \"id\", None),\n",
    "            \"source\": h.payload.get(\"source\"),\n",
    "            \"page\": h.payload.get(\"page\"),\n",
    "            \"chunk_index\": h.payload.get(\"chunk_index\")\n",
    "        })\n",
    "\n",
    "    # Step 4: Store retrieved chunks in RAG state\n",
    "    state.results = results\n",
    "    print(f\"Retrieved {len(results)} chunks.\")\n",
    "    print(\"\\n\\033[1;42m--- End Retrieval from Qdrant process ---\\033[0m\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a6917",
   "metadata": {},
   "source": [
    "### Evaluate Retrieval Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_ranked(state: RAGState, top_k: int = 5,\n",
    "                              relevance_threshold: float = 0.4,\n",
    "                              ndcg_threshold: float = 0.6,\n",
    "                              mrr_threshold: float = 0.5,\n",
    "                              use_qdrant_scores: bool = False) -> str:\n",
    "    # Evaluate retrieval quality using ranking metrics (nDCG@k, MRR).\n",
    "    # Returns 'generate' if the retrieval quality is good enough,\n",
    "    # otherwise 'rewrite' to trigger a query refinement step.\n",
    "\n",
    "    print(\"\\n\\033[1;42m--- Start Retrieval Evaluation Process ---\\033[0m\")\n",
    "    if not state.results:\n",
    "        print(\"No retrieved chunks to evaluate. Query rewrite required.\")\n",
    "        return \"rewrite\"\n",
    "\n",
    "    print(\"Evaluating retrieval quality using ranking metrics...\")\n",
    "    \n",
    "    # Evaluate the top_k retrieved texts\n",
    "    results_k = state.results[:top_k]\n",
    "    texts = [r[\"text\"] for r in results_k]\n",
    "\n",
    "    # Step 1: Encode the query vector using RTEB model (bge-m3)\n",
    "    query_vec = bge.encode([state.query], normalize_embeddings=True)[0]\n",
    "\n",
    "    # Step 2: Encode the retrieved chunks\n",
    "    retrieved_vecs = bge.encode(texts, normalize_embeddings=True)\n",
    "\n",
    "    # Step 3: Compute cosine similarity for each chunk\n",
    "    sims = np.dot(retrieved_vecs, query_vec)\n",
    "\n",
    "    # Step 4: Derive relevance labels (1 if above threshold, else 0)\n",
    "    relevance = (sims >= relevance_threshold).astype(int)\n",
    "\n",
    "    # Step 5: Compute ranking metrics (scores-predicted value vs relevance)\n",
    "    preds = np.array([r[\"score\"] for r in results_k]) if use_qdrant_scores else sims\n",
    "    ndcg = ndcg_score([relevance], [preds])\n",
    "    \n",
    "    if np.any(relevance == 1):\n",
    "        first_relevant_idx = int(np.argmax(relevance == 1))\n",
    "        reciprocal_rank = 1.0 / (first_relevant_idx + 1)\n",
    "    else:\n",
    "        reciprocal_rank = 0.0\n",
    "\n",
    "    print(f\"nDCG@{top_k}: {ndcg:.3f}, MRR: {reciprocal_rank:.3f}\")\n",
    "\n",
    "    # Step 6: Decision logic\n",
    "    if ndcg >= ndcg_threshold or reciprocal_rank >= mrr_threshold:\n",
    "        print(\"Retrieval ranking is satisfactory. Proceeding to generation.\")\n",
    "        print(\"\\n\\033[1;42m--- End Retrieval Evaluation Process ---\\033[0m\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"Retrieval ranking is poor. Triggering query rewrite.\")\n",
    "        print(\"\\n\\033[1;42m--- End Retrieval Evaluation Process ---\\033[0m\")\n",
    "        return \"rewrite\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b803a56",
   "metadata": {},
   "source": [
    "### Rewrite Query to Improve Retrieval Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dbfd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(state: RAGState) -> RAGState:\n",
    "    # Use Llama3 to rephrase the query semantically while keeping intent.\n",
    "    print(\"\\n\\033[1;42m--- Start Query Rewriting ---\\033[0m\")\n",
    "    prompt = f\"\"\"\n",
    "    You are a query rewriter for a retrieval system.\n",
    "    Rephrase the following query to improve retrieval quality\n",
    "    without changing its meaning or intent.\n",
    "\n",
    "    Query:\n",
    "    \"{state.query}\"\n",
    "    \"\"\"\n",
    "\n",
    "    new_query = llama3.generate(prompt)  # pseudo-call\n",
    "    new_query = (new_query or \"\").strip()\n",
    "\n",
    "    print(f\"\"\"\n",
    "          Rewritten query (attempt {state.retry_count + 1}):\n",
    "          \"{new_query}\"\n",
    "          \"\"\")\n",
    "    state.query = new_query\n",
    "    print(\"\\n\\033[1;42m--- End Query Rewriting ---\\033[0m\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b7d573",
   "metadata": {},
   "source": [
    "### Retrieval Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e74c7",
   "metadata": {},
   "source": [
    "Loop stops when number of rewrites = 5 to limit the response time and endless querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_loop(state: RAGState, max_retries: int = 5, top_k: int = 5) -> RAGState:\n",
    "    # Full retrieval + evaluation + rewrite loop to prevent hallucination.\n",
    "    while state.retry_count <= max_retries:\n",
    "        print(f\"\\n[Attempt {state.retry_count + 1}] Retrieving and evaluating...\")\n",
    "        \n",
    "        # Step 1: Retrieve\n",
    "        state = retrieve_from_qdrant(state, top_k=top_k)\n",
    "\n",
    "        # Step 2: Evaluate\n",
    "        result = evaluate_retrieval_ranked(state, top_k=top_k)\n",
    "\n",
    "        if result == \"generate\":\n",
    "            print(\"Retrieval sufficient to proceed LLM generation.\")\n",
    "            return state  # Pass to generation stage\n",
    "\n",
    "        # Step 3: If evaluation fails, rewrite query\n",
    "        if state.retry_count < max_retries:\n",
    "            print(\"Rewriting query and retrying retrieval...\")\n",
    "            state = rewrite_query(state)\n",
    "            state.retry_count += 1\n",
    "        else:\n",
    "            print(\"Retrieval failed after max attempts. No relevant info found.\")\n",
    "            state.answer = \"I'm sorry, but I couldn’t find relevant information about Paul from the database.\"\n",
    "            return state\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f089fdea",
   "metadata": {},
   "source": [
    "## Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama3 = Ollama(model=\"llama3\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e46b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state: RAGState, model=llama3, max_context: int = 5) -> RAGState:\n",
    "    \"\"\"\n",
    "    Generate an answer using retrieved chunks as context.\n",
    "    Uses a local Llama3 model (via Ollama) for answer generation.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\\033[1;42m--- Start Generating Answer ---\\033[0m\")\n",
    "    # Step 1: Retrieve top-k context chunks (from previous retrieval step)\n",
    "    if not state.results:\n",
    "        print(\"No retrieval results found.\")\n",
    "        state.answer = \"I'm sorry, but I couldn’t find relevant information about Paul from the database.\"\n",
    "        state.status = \"no_result\"\n",
    "        return state\n",
    "\n",
    "    # Limit context to top N results for efficiency\n",
    "    # Prevent dict that is not text\n",
    "    top_contexts_raw = [r.get(\"text\", \"\") for r in state.results[:max_context]]\n",
    "    top_contexts = [\n",
    "        (c.get(\"text\", \"\") if isinstance(c, dict) else c) for c in top_contexts_raw\n",
    "    ]\n",
    "\n",
    "    combined_context = \"\\n\\n\".join(top_contexts)\n",
    "\n",
    "    # Step 2: Build a system + user prompt\n",
    "    prompt = textwrap.dedent(f\"\"\"\n",
    "    You are a precise and concise AI assistant specialized in retrieval-augmented generation.\n",
    "    Use the following context extracted from trusted documents to answer the user's query accurately.\n",
    "    If the context does not contain sufficient information, clearly say so without hallucinating.\n",
    "\n",
    "    --- Context ---\n",
    "    {combined_context}\n",
    "\n",
    "    --- Question ---\n",
    "    {state.query}\n",
    "\n",
    "    --- Instruction ---\n",
    "    1. Base your answer only on the given context.\n",
    "    2. DO NOT invent facts not present in the documents(context).\n",
    "    3. If unsure, say \"I'm sorry, but I couldn’t find relevant information about Paul from the database.\"\n",
    "    4. Return your final answer clearly and concisely.\n",
    "    \"\"\")\n",
    "\n",
    "    try:\n",
    "        response = model.invoke(prompt)\n",
    "        answer_text = response.strip()\n",
    "        state.status = \"success\"\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect GenAI Model: {e}\")\n",
    "        state.answer = \"\"\"\n",
    "                Error: failed to generate answer due to model connection or runtime issue.\n",
    "                Please contact Paul for resolving technical issue.\n",
    "            \"\"\"\n",
    "        state.status = \"error\"\n",
    "        return state  # Stop pipeline here if error\n",
    "        \n",
    "\n",
    "    # Step 3: Store generated answer\n",
    "    state.answer = answer_text\n",
    "\n",
    "    # Step 4: Log summary\n",
    "    print(\" \")\n",
    "    print(\"\\n====== Generated Answer ======\")\n",
    "    print(\" \")\n",
    "    print(answer_text)\n",
    "    print(\" \")\n",
    "    print(\"================================\\n\")\n",
    "    print(\" \")\n",
    "\n",
    "    print(\"\\n\\033[1;42m--- End Generating Answer ---\\033[0m\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e019f",
   "metadata": {},
   "source": [
    "### Answer Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9c7fd",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics\n",
    "1. Semantic Evaluation using DeepEval Faithfulness and Answer Relevancy\n",
    "  - Faithfulness: Is answer based on the context retrieved by the retrieval model?\n",
    "  - Answer Relevancy: Is answer relevant to the user query(question)?\n",
    "\n",
    "2. Liguistic / Ethical Evaluation using custom metrics\n",
    "  - Grammar: Grammar Evaluation\n",
    "  - Fluency: Fluency Evaluation\n",
    "  - Coherence: Evaluating consistency of logical structure between sentences\n",
    "  - Conciseness: Evaluating the unnecessary repeatition\n",
    "  - Toxicity and Bias: Ethical use of words and sentences Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f9340",
   "metadata": {},
   "source": [
    "#### Tier 1 - Semantic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c939f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "faith_model = OllamaModel(model=\"llama3:instruct\")\n",
    "relev_model = OllamaModel(model=\"llama3:instruct\")\n",
    "\n",
    "faith_metric = FaithfulnessMetric(model=faith_model, threshold=0.7)\n",
    "relev_metric = AnswerRelevancyMetric(model=relev_model, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f02dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer_tier1(state: RAGState, faith_thresh=0.7, relev_thresh=0.7) -> str:\n",
    "    print(\"\\n\\033[1;42m--- Start Evaluating Answer (Tier 1) ---\\033[0m\")\n",
    "\n",
    "    if not state.answer or not state.results:\n",
    "        print(\"No generated answer or context found for Tier 1 evaluation.\")\n",
    "        return \"rewrite\"\n",
    "\n",
    "    # --- Context aggregation ---\n",
    "    context = \"\\n\".join([r[\"text\"] for r in state.results])\n",
    "    test_case = LLMTestCase(\n",
    "        input=state.query,\n",
    "        actual_output=state.answer,\n",
    "        retrieval_context=[context],\n",
    "        expected_output=None\n",
    "    )\n",
    "\n",
    "    # --- Step 1. Faithfulness ---\n",
    "    print(\"Evaluating Faithfulness...\")\n",
    "    try:\n",
    "        faith_result = evaluate(test_cases=[test_case], metrics=[faith_metric])\n",
    "\n",
    "        # DeepEval 3.6.7 → dict-like structure\n",
    "        if isinstance(faith_result, dict):\n",
    "            test_case_data = faith_result.get(\"test_cases\", [])[0]\n",
    "            metrics_results = test_case_data.get(\"metrics_results\", [])\n",
    "            # Faithfulness metric 찾기\n",
    "            faith_score = next(\n",
    "                (m.get(\"score\", 0) for m in metrics_results if \"Faith\" in m.get(\"metric_name\", \"\")),\n",
    "                0\n",
    "            )\n",
    "        else:\n",
    "            raise AttributeError(\"Unexpected DeepEval result structure (expected dict)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Faithfulness evaluation failed: {e}\")\n",
    "        return \"rewrite\"\n",
    "\n",
    "    print(f\"Faithfulness Score: {faith_score:.3f}\")\n",
    "    if faith_score < faith_thresh:\n",
    "        print(\"Faithfulness failed → rewrite required.\")\n",
    "        return \"rewrite\"\n",
    "\n",
    "    # --- Step 2. Relevancy ---\n",
    "    print(\"Faithfulness passed ✓\\nEvaluating Relevancy...\")\n",
    "    try:\n",
    "        relev_result = evaluate(test_cases=[test_case], metrics=[relev_metric])\n",
    "\n",
    "        if isinstance(relev_result, dict):\n",
    "            test_case_data = relev_result.get(\"test_cases\", [])[0]\n",
    "            metrics_results = test_case_data.get(\"metrics_results\", [])\n",
    "            relev_score = next(\n",
    "                (m.get(\"score\", 0) for m in metrics_results if \"Relev\" in m.get(\"metric_name\", \"\")),\n",
    "                0\n",
    "            )\n",
    "        else:\n",
    "            raise AttributeError(\"Unexpected DeepEval result structure (expected dict)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Relevancy evaluation failed: {e}\")\n",
    "        return \"rewrite\"\n",
    "\n",
    "    print(f\"Relevancy Score: {relev_score:.3f}\")\n",
    "    if relev_score < relev_thresh:\n",
    "        print(\"Relevancy failed → rewrite required.\")\n",
    "        return \"rewrite\"\n",
    "\n",
    "    print(\"Passed Tier 1 - Semantic Evaluation\")\n",
    "    print(\"\\033[1;42m--- End Evaluating Answer (Tier 1) ---\\033[0m\")\n",
    "    return \"pass\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_answer_tier1(state: RAGState,\n",
    "#                           faith_thresh: float = 0.7,\n",
    "#                           relev_thresh: float = 0.7) -> str:\n",
    "#     \"\"\"\n",
    "#     Tier 1 evaluation for DeepEval v3.x\n",
    "#     - Checks semantic validity (Faithfulness + Answer Relevancy)\n",
    "#     - Returns 'pass' or 'rewrite'\n",
    "#     \"\"\"\n",
    "    \n",
    "#     print(\"\\n\\033[1;42m--- Start Evaluating Answer (Tier 1) ---\\033[0m\")\n",
    "\n",
    "#     if not state.answer or not state.results:\n",
    "#         print(\"No generated answer or context found.\")\n",
    "#         return \"rewrite\"\n",
    "\n",
    "#     context = \"\\n\".join([r[\"text\"] for r in state.results])\n",
    "\n",
    "#     # Define evaluation test case\n",
    "#     test_case = LLMTestCase(\n",
    "#         input=state.query,\n",
    "#         actual_output=state.answer,\n",
    "#         retrieval_context=[context]\n",
    "#         expected_output=None\n",
    "#     )\n",
    "\n",
    "#     print(\"Evaluating Tier 1 - Faithfulness & Relevancy (DeepEval v3.x)...\")\n",
    "\n",
    "#     try:\n",
    "#         result = evaluate(\n",
    "#             test_cases=[test_case],\n",
    "#             metrics=[faith_metric, relev_metric],\n",
    "#             )\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"DeepEval evaluate() failed: {e}\")\n",
    "#         return \"rewrite\"\n",
    "\n",
    "#     faith_score, relev_score = None, None\n",
    "\n",
    "#     try:\n",
    "#         if hasattr(result, \"metrics\") and isinstance(result.metrics, list):\n",
    "#             for metric in result.metrics:\n",
    "#                 if \"Faith\" in metric.name:\n",
    "#                     faith_score = getattr(metric, \"score\", 0)\n",
    "#                 elif \"Relev\" in metric.name:\n",
    "#                     relev_score = getattr(metric, \"score\", 0)\n",
    "#         else:\n",
    "#             raise AttributeError(\"Unexpected DeepEval v3.x result structure\")\n",
    "\n",
    "#     except Exception as ee:\n",
    "#         print(f\"Could not extract metric scores (DeepEval 3.x): {ee}\")\n",
    "#         return \"rewrite\"\n",
    "\n",
    "#     faith_score = faith_score or 0.0\n",
    "#     relev_score = relev_score or 0.0\n",
    "\n",
    "#     print(f\"Faithfulness: {faith_score:.3f} | Relevancy: {relev_score:.3f}\")\n",
    "\n",
    "#     if faith_score >= faith_thresh and relev_score >= relev_thresh:\n",
    "#         print(\"Passed Tier 1 - Semantic Evaluation\")\n",
    "#         print(\"\\n\\033[1;42m--- End Evaluating Answer (Tier 1) ---\\033[0m\")\n",
    "#         return \"pass\"\n",
    "#     else:\n",
    "#         print(\"Failed Tier 1 - Sending back to rewrite\")\n",
    "#         print(\"\\n\\033[1;42m--- End Evaluating Answer (Tier 1) ---\\033[0m\")\n",
    "#         return \"rewrite\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3280d7c3",
   "metadata": {},
   "source": [
    "#### Tier 2 - Linguistic / Ethical Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialMetric(BaseMetric):\n",
    "    def _get_score(self, prompt):\n",
    "        res = llama3.generate([prompt]).generations[0][0].text\n",
    "        m = re.search(r\"([0-9]*\\.?[0-9]+)\", res)\n",
    "        return float(m.group(1)) if m else 0.0\n",
    "\n",
    "class GrammarMetric(SequentialMetric):\n",
    "    def measure(self, q,a,c=None): return self._get_score(f\"Rate grammar (0-1): {a}\")\n",
    "class FluencyMetric(SequentialMetric):\n",
    "    def measure(self, q,a,c=None): return self._get_score(f\"Rate fluency (0-1): {a}\")\n",
    "class CoherenceMetric(SequentialMetric):\n",
    "    def measure(self, q,a,c=None): return self._get_score(f\"Rate coherence (0-1): {a}\")\n",
    "class ConcisenessMetric(SequentialMetric):\n",
    "    def measure(self, q,a,c=None): return self._get_score(f\"Rate conciseness (0-1): {a}\")\n",
    "class ToxicityMetric(SequentialMetric):\n",
    "    def measure(self, q,a,c=None): return self._get_score(f\"Rate toxicity (0 safe-1 toxic): {a}\")\n",
    "class BiasMetric(SequentialMetric):\n",
    "    def measure(self, q,a,c=None): return self._get_score(f\"Rate bias (0 neutral-1 biased): {a}\")\n",
    "\n",
    "def evaluate_answer_tier2(state: RAGState) -> str:\n",
    "    print(\"\\n\\033[1;42m--- Start Evaluating Answer (Tier 2) ---\\033[0m\")\n",
    "    metrics = [\n",
    "        (\"Grammar\", GrammarMetric(), 0.75, True),\n",
    "        (\"Fluency\", FluencyMetric(), 0.75, True),\n",
    "        (\"Coherence\", CoherenceMetric(), 0.75, True),\n",
    "        (\"Conciseness\", ConcisenessMetric(), 0.6, True),\n",
    "        (\"Toxicity\", ToxicityMetric(), 0.2, False),\n",
    "        (\"Bias\", BiasMetric(), 0.3, False),\n",
    "    ]\n",
    "\n",
    "    for name, metric, thresh, greater in metrics:\n",
    "        try:\n",
    "            score = metric.measure(state.query, state.answer)\n",
    "            print(f\"{name}: {score:.3f}\")\n",
    "            if (greater and score < thresh) or (not greater and score > thresh):\n",
    "                print(f\"{name} failed → rewrite required.\")\n",
    "                return \"rewrite\"\n",
    "        except Exception as e:\n",
    "            print(f\"{name} metric failed: {e}\")\n",
    "            return \"rewrite\"\n",
    "\n",
    "    print(\"Passed Tier 2 ✓\")\n",
    "    return \"final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e158c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GrammarMetric(BaseMetric):\n",
    "#     def __init__(self, model=None, threshold=0.75):\n",
    "#         super().__init__()\n",
    "#         self.model = llama3\n",
    "#         self.threshold = threshold\n",
    "#         self.name = \"Grammar Quality\"\n",
    "\n",
    "#     def measure(self, question, answer, context=None):\n",
    "#         prompt = f\"\"\"\n",
    "#         Evaluate the grammar and sentence correctness of this answer.\n",
    "#         Give a score between 0 (poor) and 1 (perfect grammar).\n",
    "#         Answer:\n",
    "#         {answer}\n",
    "#         \"\"\"\n",
    "#         raw_score = self.model.generate([prompt]).generations[0][0].text\n",
    "#         try:\n",
    "#             score = float(raw_score.strip())\n",
    "#         except Exception:\n",
    "#             match = re.search(r\"([0-9]*\\.?[0-9]+)\", raw_score)\n",
    "#             score = float(match.group(1)) if match else 0.0\n",
    "#         return float(score)\n",
    "\n",
    "# class FluencyMetric(BaseMetric):\n",
    "#     def __init__(self, model=None, threshold=0.75):\n",
    "#         super().__init__()\n",
    "#         self.model = llama3\n",
    "#         self.threshold = threshold\n",
    "#         self.name = \"Fluency\"\n",
    "\n",
    "#     def measure(self, question, answer, context=None):\n",
    "#         prompt = f\"\"\"\n",
    "#         Rate the fluency and naturalness of this answer from 0 to 1.\n",
    "#         Avoid judging content, only evaluate smoothness and readability.\n",
    "#         Answer:\n",
    "#         {answer}\n",
    "#         \"\"\"\n",
    "#         raw_score = self.model.generate([prompt]).generations[0][0].text\n",
    "#         try:\n",
    "#             score = float(raw_score.strip())\n",
    "#         except Exception:\n",
    "#             match = re.search(r\"([0-9]*\\.?[0-9]+)\", raw_score)\n",
    "#             score = float(match.group(1)) if match else 0.0\n",
    "#         return float(score)\n",
    "\n",
    "# class CoherenceMetric(BaseMetric):\n",
    "#     def __init__(self, model=None, threshold=0.75):\n",
    "#         super().__init__()\n",
    "#         self.model = llama3\n",
    "#         self.threshold = threshold\n",
    "#         self.name = \"Coherence\"\n",
    "\n",
    "#     def measure(self, question, answer, context=None):\n",
    "#         prompt = f\"\"\"\n",
    "#         Evaluate the logical coherence and flow of the following answer.\n",
    "#         Give a score from 0 (disjointed) to 1 (highly coherent).\n",
    "#         Answer:\n",
    "#         {answer}\n",
    "#         \"\"\"\n",
    "#         raw_score = self.model.generate([prompt]).generations[0][0].text\n",
    "#         try:\n",
    "#             score = float(raw_score.strip())\n",
    "#         except Exception:\n",
    "#             match = re.search(r\"([0-9]*\\.?[0-9]+)\", raw_score)\n",
    "#             score = float(match.group(1)) if match else 0.0\n",
    "#         return float(score)\n",
    "\n",
    "# class ConcisenessMetric(BaseMetric):\n",
    "#     def __init__(self, model=None, threshold=0.6):\n",
    "#         super().__init__()\n",
    "#         self.model = llama3\n",
    "#         self.threshold = threshold\n",
    "#         self.name = \"Conciseness\"\n",
    "\n",
    "#     def measure(self, question, answer, context=None):\n",
    "#         prompt = f\"\"\"\n",
    "#         Evaluate how concise this answer is.\n",
    "#         Penalize redundancy or overly long explanations.\n",
    "#         Score from 0 (verbose) to 1 (clear and concise).\n",
    "#         Answer:\n",
    "#         {answer}\n",
    "#         \"\"\"\n",
    "#         raw_score = self.model.generate([prompt]).generations[0][0].text\n",
    "#         try:\n",
    "#             score = float(raw_score.strip())\n",
    "#         except Exception:\n",
    "#             match = re.search(r\"([0-9]*\\.?[0-9]+)\", raw_score)\n",
    "#             score = float(match.group(1)) if match else 0.0\n",
    "#         return float(score)\n",
    "\n",
    "# class ToxicityMetric(BaseMetric):\n",
    "#     def __init__(self, model=None, threshold=0.2):\n",
    "#         super().__init__()\n",
    "#         self.model = llama3\n",
    "#         self.threshold = threshold\n",
    "#         self.name = \"Toxicity\"\n",
    "\n",
    "#     def measure(self, question, answer, context=None):\n",
    "#         prompt = f\"\"\"\n",
    "#         Detect any offensive, hateful, or toxic language in the following answer.\n",
    "#         Give a score from 0 (completely safe) to 1 (very toxic).\n",
    "#         Answer:\n",
    "#         {answer}\n",
    "#         \"\"\"\n",
    "#         raw_score = self.model.generate([prompt]).generations[0][0].text\n",
    "#         try:\n",
    "#             score = float(raw_score.strip())\n",
    "#         except Exception:\n",
    "#             match = re.search(r\"([0-9]*\\.?[0-9]+)\", raw_score)\n",
    "#             score = float(match.group(1)) if match else 0.0\n",
    "#         return float(score)\n",
    "\n",
    "# class BiasMetric(BaseMetric):\n",
    "#     def __init__(self, model=None, threshold=0.3):\n",
    "#         super().__init__()\n",
    "#         self.model = llama3\n",
    "#         self.threshold = threshold\n",
    "#         self.name = \"Bias\"\n",
    "\n",
    "#     def measure(self, question, answer, context=None):\n",
    "#         prompt = f\"\"\"\n",
    "#         Evaluate if this answer contains biased or discriminatory language.\n",
    "#         Give a score from 0 (neutral) to 1 (biased).\n",
    "#         Answer:\n",
    "#         {answer}\n",
    "#         \"\"\"\n",
    "#         raw_score = self.model.generate([prompt]).generations[0][0].text\n",
    "#         try:\n",
    "#             score = float(raw_score.strip())\n",
    "#         except Exception:\n",
    "#             match = re.search(r\"([0-9]*\\.?[0-9]+)\", raw_score)\n",
    "#             score = float(match.group(1)) if match else 0.0\n",
    "#         return float(score)\n",
    "\n",
    "# # --- Tier 2 Evaluation Logic ---\n",
    "# def evaluate_answer_tier2(state: RAGState) -> str:\n",
    "#     \"\"\"\n",
    "#     Tier 2 evaluation:\n",
    "#     - Checks Grammar, Fluency, Coherence, Conciseness, Toxicity, Bias\n",
    "#     - Returns 'final' or 'rewrite'\n",
    "#     \"\"\"\n",
    "#     print(\"\\n\\033[1;42m--- Start Evaluating Answer (Tier 2) ---\\033[0m\")\n",
    "\n",
    "#     if not state.answer:\n",
    "#         print(\"No answer found for Tier 2 evaluation.\")\n",
    "#         return \"rewrite\"\n",
    "\n",
    "#     print(\"Evaluating Tier 2 - linguistic/ethical evaluation...\")\n",
    "\n",
    "#     metrics = [\n",
    "#         GrammarMetric(), FluencyMetric(), CoherenceMetric(),\n",
    "#         ConcisenessMetric(), ToxicityMetric(), BiasMetric()\n",
    "#     ]\n",
    "\n",
    "#     context = \"\\n\".join([r[\"text\"] for r in state.results]) if state.results else \"\"\n",
    "#     question = state.query or \"General query\"\n",
    "\n",
    "#     # Evaluate\n",
    "#     scores = {}\n",
    "#     for m in metrics:\n",
    "#         try:\n",
    "#             score = m.measure(question, state.answer, context)\n",
    "#             scores[m.name] = score\n",
    "#         except Exception as e:\n",
    "#             scores[m.name] = 0.0\n",
    "#             print(f\"Metric {m.name} failed: {e}\")\n",
    "\n",
    "#     # Log scores\n",
    "#     for k, v in scores.items():\n",
    "#         print(f\"{k}: {v:.3f}\")\n",
    "\n",
    "#     # Determine pass/fail\n",
    "#     if (scores[\"Grammar Quality\"] >= 0.75 and\n",
    "#         scores[\"Fluency\"] >= 0.75 and\n",
    "#         scores[\"Coherence\"] >= 0.75 and\n",
    "#         scores[\"Conciseness\"] >= 0.6 and\n",
    "#         scores[\"Toxicity\"] <= 0.2 and\n",
    "#         scores[\"Bias\"] <= 0.3):\n",
    "#         print(\"Passed Tier 2 - Linguistic/Ethical Evaluation\")\n",
    "#         print(\"\\n\\033[1;42m--- End Evaluating Answer (Tier 2) ---\\033[0m\")\n",
    "#         return \"final\"\n",
    "#     else:\n",
    "#         print(\"Failed Tier 2 - Sending back to rewrite.\")\n",
    "#         print(\"\\n\\033[1;42m--- End Evaluating Answer (Tier 2) ---\\033[0m\")\n",
    "#         return \"rewrite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56999f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_answer_evaluation(state: RAGState) -> RAGState:\n",
    "    # Tier 1: Faithfulness → Relevancy\n",
    "    while True:\n",
    "        result = evaluate_answer_tier1(state)\n",
    "        if result == \"pass\":\n",
    "            print(\"Proceeding to Tier 2.\")\n",
    "            break\n",
    "        state = rewrite_answer(state, reason=\"tier1\")\n",
    "\n",
    "    # Tier 2: Sequential Metrics\n",
    "    while True:\n",
    "        result = evaluate_answer_tier2(state)\n",
    "        if result == \"final\":\n",
    "            break\n",
    "        state = rewrite_answer(state, reason=\"tier2\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1ad224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def full_answer_evaluation(state: RAGState,\n",
    "#                            max_semantic_retries: int = 5,\n",
    "#                            max_linguistic_retries: int = 3) -> RAGState:\n",
    "#     \"\"\"\n",
    "#     Full evaluation pipeline:\n",
    "#     - Tier 1 + Tier 2\n",
    "#     - Adaptive retry: max total rewrites = 8\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Tier 1 - Semantic evaluation\n",
    "#     while state.retry_count < max_semantic_retries:\n",
    "#         print(f\"\\n\\033[1;44m[Semantic Eval Attempt {state.retry_count + 1}]\\033[0m\")\n",
    "\n",
    "#         tier1_result = evaluate_answer_tier1(state)\n",
    "#         if tier1_result == \"pass\":\n",
    "#             print(\"Passed Tier 1. Moving to Tier 2.\")\n",
    "#             break\n",
    "\n",
    "#         # Rewrite on fail\n",
    "#         if tier1_result == \"rewrite\":\n",
    "#             state = rewrite_answer(state, reason=\"semantic\")\n",
    "#         state.retry_count += 1\n",
    "\n",
    "#     else:\n",
    "#         print(\"Tier 1 failed after max retries.\")\n",
    "#         state.answer = (\n",
    "#             \"I'm sorry, but I couldn’t generate a factually accurate and relevant answer. Could you try again?\"\n",
    "#         )\n",
    "#         return state\n",
    "\n",
    "#     # Tier 2 - Linguistic/Ethical evaluation\n",
    "#     linguistic_retries = 0\n",
    "#     while linguistic_retries < max_linguistic_retries:\n",
    "#         print(f\"\\n\\033[1;44m[Linguistic Eval Attempt {linguistic_retries + 1}]\\033[0m\")\n",
    "\n",
    "#         tier2_result = evaluate_answer_tier2(state)\n",
    "#         if tier2_result == \"final\":\n",
    "#             print(\"Passed Tier 2. Answer fully verified.\")\n",
    "#             return state\n",
    "\n",
    "#         # Rewrite & adjust retry counters\n",
    "#         if tier2_result == \"rewrite\":\n",
    "#             state = rewrite_answer(state, reason=\"linguistic\")\n",
    "#         linguistic_retries += 1\n",
    "\n",
    "#         # Deduct semantic retry budget\n",
    "#         if state.retry_count < max_semantic_retries:\n",
    "#             state.retry_count += 1\n",
    "#             print(f\"Deducted 1 semantic retry (used {state.retry_count}/{max_semantic_retries})\")\n",
    "\n",
    "#         # Abort if total rewrite exceeds 8\n",
    "#         if state.retry_count >= max_semantic_retries:\n",
    "#             print(\"Answer generation total attempts exceeded 8. Aborting.\")\n",
    "#             state.answer = (\n",
    "#                 \"I'm sorry, but I couldn’t refine the answer further without risking factual distortion. Could you try again?\"\n",
    "#             )\n",
    "#             return state\n",
    "\n",
    "#     print(\"Minor linguistic issues remain. Accepting best-effort answer.\")\n",
    "#     return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_answer(state: RAGState, reason: str = \"generic\") -> RAGState:\n",
    "    \"\"\"\n",
    "    Dynamically rewrites an answer depending on failure reason.\n",
    "    - Semantic issue → factual or contextual re-grounding\n",
    "    - Linguistic/Ethical issue → grammar, tone, or style refinement\n",
    "    \"\"\"\n",
    "    print(\"\\n\\033[1;42m--- Start Rewriting Answer ---\\033[0m\")\n",
    "\n",
    "    if not state.answer:\n",
    "        print(\"No existing answer to rewrite.\")\n",
    "        return state\n",
    "\n",
    "    # --- Semantic Rewriting ---\n",
    "    if reason == \"tier1\":\n",
    "        prompt = f\"\"\"\n",
    "        You are a semantic rewriter for factual accuracy.\n",
    "        The following answer was generated for a question but failed semantic evaluation.\n",
    "        Please rephrase it so that it is more faithful to the context and relevant to the question,\n",
    "        without hallucinating or introducing new information.\n",
    "\n",
    "        --- Question ---\n",
    "        {state.query}\n",
    "\n",
    "        --- Original Answer ---\n",
    "        {state.answer}\n",
    "\n",
    "        --- Instruction ---\n",
    "        1. Focus on factual alignment with the retrieved context.\n",
    "        2. Keep only verified details; remove speculative or unrelated content.\n",
    "        3. Maintain a professional, concise tone.\n",
    "        \"\"\"\n",
    "\n",
    "    # --- Linguistic / Ethical Rewriting ---\n",
    "    elif reason == \"tier2\":\n",
    "        prompt = f\"\"\"\n",
    "        You are an answer refinement assistant for linguistic and ethical improvements.\n",
    "        The following answer was grammatically weak, verbose, or stylistically inconsistent.\n",
    "        Refine it for grammar, fluency, coherence, conciseness, and remove any biased or unsafe phrasing.\n",
    "\n",
    "        --- Question ---\n",
    "        {state.query}\n",
    "\n",
    "        --- Original Answer ---\n",
    "        {state.answer}\n",
    "\n",
    "        --- Instruction ---\n",
    "        1. Keep factual content unchanged.\n",
    "        2. Improve grammar, coherence, and readability.\n",
    "        3. Remove redundancy or biased language.\n",
    "        4. Return a single improved version.\n",
    "        \"\"\"\n",
    "\n",
    "    # --- Fallback: generic ---\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "        Refine the following answer to improve clarity and accuracy without changing meaning.\n",
    "        --- Question ---\n",
    "        {state.query}\n",
    "        --- Answer ---\n",
    "        {state.answer}\n",
    "        \"\"\"\n",
    "\n",
    "    print(f\"Rewriting answer due to {reason} issue...\")\n",
    "    try:\n",
    "        refined = llama3.invoke(prompt).strip()\n",
    "        print(\"\\n--- Rewritten Answer ---\\n\", refined)\n",
    "        state.answer = refined\n",
    "        state.status = f\"rewritten_{reason}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to rewrite answer: {e}\")\n",
    "        state.status = \"rewrite_error\"\n",
    "\n",
    "    print(\"\\n\\033[1;42m--- End Rewriting Answer ---\\033[0m\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize State\n",
    "state = RAGState()\n",
    "state.query = input(\"Enter your question: \").strip()\n",
    "print(f\"\"\"\n",
    "      Generating Answer for Your question:\n",
    "      {state.query}\n",
    "    \"\"\")\n",
    "\n",
    "# Load & Chunk PDFs (only once unless documents change)\n",
    "print(\"\\n\\033[1;42m--- Loading and Chunking Documents ---\\033[0m\")\n",
    "state = load_and_chunk(state)\n",
    "\n",
    "# Embed and Store in Qdrant (only once after load)\n",
    "print(\"\\n\\033[1;42m---Embedding and Storing Chunks in Qdrant ---\\033[0m\")\n",
    "state = embed_and_store(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdbd679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = RAGState()\n",
    "# state.query = input(\"Enter your question: \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval Loop — uses bge-m3 for retrieval, evaluation & rewriting if needed\n",
    "print(\"\\n\\033[1;42m--- Starting Retrieval Loop ---\\033[0m\")\n",
    "state = retrieval_loop(state)\n",
    "\n",
    "# Answer Generation using Llama3\n",
    "print(\"\\n\\033[1;42m--- Generating Answer ---\\033[0m\")\n",
    "state = generate_answer(state)\n",
    "\n",
    "# Check status before evaluate\n",
    "# Dual-layer Evaluation (DeepEval Tier1 + Custom Tier2)\n",
    "if getattr(state, \"status\", None) == \"error\":\n",
    "    print(\"\\n Model connection failed. Evaluation aborted.\")\n",
    "    print(\"Please contact Paul for resolving technical issue.\\n\")\n",
    "else:\n",
    "    print(\"\\n\\033[1;42m--- Evaluating Generated Answer ---\\033[0m\")\n",
    "    state = full_answer_evaluation(state)\n",
    "\n",
    "# Final Output (Answer to the question of user query)\n",
    "print(\"\\n=====================================================\")\n",
    "print(\"\")\n",
    "print(state.answer)\n",
    "print(\"\")\n",
    "print(\"=====================================================\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasongraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
